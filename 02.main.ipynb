{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD0QAOjNxUSX"
   },
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5634,
     "status": "ok",
     "timestamp": 1747708079597,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "NDEfm7s6S8oU",
    "outputId": "13db00f4-f54a-4059-a032-6d31135baee3"
   },
   "outputs": [],
   "source": [
    "!pip uninstall numpy -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11618,
     "status": "ok",
     "timestamp": 1747708091216,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "0zsz6mj4TDJq",
    "outputId": "43dd3c97-d0ed-4a2e-f470-e9fbb7177d06"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1747708103230,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "QSkE59MhTLsi",
    "outputId": "60343094-1114-465a-c96e-fb7ebe9a9646"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5610,
     "status": "ok",
     "timestamp": 1747708108848,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "Fyydteit9nwd",
    "outputId": "8ffa0b33-4c6b-43a1-ff49-0d7f0b567b34"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda_streams():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available.\")\n",
    "    else:\n",
    "        current_stream = torch.cuda.current_stream()\n",
    "        print(f\"Current CUDA stream: {current_stream}\")\n",
    "\n",
    "check_cuda_streams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1747708113294,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "s0j6gobDGApN",
    "outputId": "b169283d-4e1d-4c18-da30-ab5f9fd2ee8a"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1747708118094,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "awts07Mw92Cp",
    "outputId": "956347a9-57c7-47b0-81e8-f93de0a6ff74"
   },
   "outputs": [],
   "source": [
    "import psutil\n",
    "\n",
    "def check_cpu_info():\n",
    "    # Get the number of physical cores\n",
    "    physical_cores = psutil.cpu_count(logical=False)\n",
    "    # Get the number of logical (including hyperthreaded) cores\n",
    "    logical_cores = psutil.cpu_count(logical=True)\n",
    "\n",
    "    print(f\"Number of Physical Cores: {physical_cores}\")\n",
    "    print(f\"Number of Logical Cores (Threads): {logical_cores}\")\n",
    "\n",
    "    # Get CPU frequency\n",
    "    cpu_freq = psutil.cpu_freq()\n",
    "    print(f\"CPU Frequency: {cpu_freq.current:.2f} MHz\")\n",
    "\n",
    "    # Get CPU utilization\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU Utilization: {cpu_usage:.2f}%\")\n",
    "\n",
    "check_cpu_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12291,
     "status": "ok",
     "timestamp": 1747708131153,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "Rn5Kg0GoS6wj",
    "outputId": "68af1341-5183-4539-e706-fb677b80d625"
   },
   "outputs": [],
   "source": [
    "!pip install -q condacolab\n",
    "import condacolab\n",
    "condacolab.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41712,
     "status": "ok",
     "timestamp": 1747708189398,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "yCy2j4yij7Wu",
    "outputId": "79c09b2b-3f38-4640-9a69-0c6363530a6c"
   },
   "outputs": [],
   "source": [
    "!conda update -n base -c defaults conda\n",
    "!conda install -n base -c conda-forge mamba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 113,
     "status": "ok",
     "timestamp": 1747708189513,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "SXjgpmq7gDW7",
    "outputId": "ddd34766-840a-462f-8522-2bd121928ba0"
   },
   "outputs": [],
   "source": [
    "!which conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34279,
     "status": "ok",
     "timestamp": 1747708223793,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "WvYn0t3PzZRp",
    "outputId": "545cd078-1896-4297-8662-ff38c60ac733"
   },
   "outputs": [],
   "source": [
    "!conda install -n base -c conda-forge ecole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141861,
     "status": "ok",
     "timestamp": 1747708365706,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "rvlEiNCDzsB0",
    "outputId": "dd8f0426-ccbb-4f7f-da37-a7374ce81fc7"
   },
   "outputs": [],
   "source": [
    "!conda install -n base -c conda-forge scip=8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1747708365717,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "5uMDJDM5R1K-",
    "outputId": "0acdbea4-6115-4564-fcdb-fac0ef7d24f2"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(numpy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 77,
     "status": "ok",
     "timestamp": 1747708365795,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "XfIq3kGkQmXS",
    "outputId": "a25d9a1f-171d-4c94-8bb9-10a8bd5c72ea"
   },
   "outputs": [],
   "source": [
    "import ecole\n",
    "print(dir(ecole))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 143473,
     "status": "ok",
     "timestamp": 1747708509227,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "EPYmxLObHeyh",
    "outputId": "40ff0126-c845-41d9-b276-ff56fab787ed"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15457,
     "status": "ok",
     "timestamp": 1747708524687,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "o4JWPIWBaJ8_",
    "outputId": "9ab73ff7-ce77-418c-c541-eaba5d7c8b27"
   },
   "outputs": [],
   "source": [
    "!pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4642,
     "status": "ok",
     "timestamp": 1747708529331,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "xkLd9zElGKX6",
    "outputId": "2046a3bc-ba88-4638-d1aa-2ba488495a9e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "print(torch.__version__)         \n",
    "print(torch.version.cuda)        \n",
    "print(torch.backends.cudnn.version())\n",
    "print(torch.cuda.is_available()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 621,
     "status": "ok",
     "timestamp": 1747708529954,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "Soyrj8foaC9F",
    "outputId": "f6fbb133-9ba9-4edf-87ed-42dbc6fd5575"
   },
   "outputs": [],
   "source": [
    "!pip list | grep torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12309,
     "status": "ok",
     "timestamp": 1747708542260,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "RZIb-hcWlDJk",
    "outputId": "257e3862-dc9e-4f9d-eafc-1b999728baf5"
   },
   "outputs": [],
   "source": [
    "!pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyscipopt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_MoqdJIRvOit"
   },
   "source": [
    "# Drive Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23874,
     "status": "ok",
     "timestamp": 1747708786672,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "zUAToAvoIoe_",
    "outputId": "3051e3ef-e0e6-4dc1-c931-77e7371a4928"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import time\n",
    "import os\n",
    "drive.flush_and_unmount()\n",
    "# Mount Google Drive if needed\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "test_file = \"/content/drive/MyDrive/drive_speed_test.bin\"\n",
    "file_size = 1024 * 1024 * 100  # 100 MB\n",
    "\n",
    "# Write Speed Test\n",
    "start_time = time.time()\n",
    "with open(test_file, \"wb\") as f:\n",
    "    f.write(os.urandom(file_size))\n",
    "write_time = time.time() - start_time\n",
    "write_speed = file_size / write_time / (1024 * 1024)  # MB/s\n",
    "print(f\"Write Speed: {write_speed:.2f} MB/s\")\n",
    "\n",
    "# Read Speed Test\n",
    "start_time = time.time()\n",
    "with open(test_file, \"rb\") as f:\n",
    "    f.read()\n",
    "read_time = time.time() - start_time\n",
    "read_speed = file_size / read_time / (1024 * 1024)  # MB/s\n",
    "print(f\"Read Speed: {read_speed:.2f} MB/s\")\n",
    "\n",
    "# Clean up\n",
    "os.remove(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1747708788767,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "WcZQgbok0i8x"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/Thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1747708789353,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "0AT5GmNFvRFK",
    "outputId": "fbbba6b1-8939-4888-c9dd-bf9c6d967ebc"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1747708789792,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "-hVt1rQVwe10",
    "outputId": "df3e74cf-0784-48c6-a59d-d45a4873650e"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tidPLGn3xCxD"
   },
   "source": [
    "# Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7222,
     "status": "ok",
     "timestamp": 1747708806125,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "k00CSMxoxCY_"
   },
   "outputs": [],
   "source": [
    "\n",
    "import gzip\n",
    "import pickle\n",
    "import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric\n",
    "\n",
    "def log(str, logfile=None):\n",
    "    str = f'[{datetime.datetime.now()}] {str}'\n",
    "    print(str)\n",
    "    if logfile is not None:\n",
    "        with open(logfile, mode='a') as f:\n",
    "            print(str, file=f)\n",
    "\n",
    "\n",
    "def pad_tensor(input_, pad_sizes, pad_value=-1e8):\n",
    "    max_pad_size = pad_sizes.max()\n",
    "    output = input_.split(pad_sizes.cpu().numpy().tolist())\n",
    "    output = torch.stack([F.pad(slice_, (0, max_pad_size-slice_.size(0)), 'constant', pad_value)\n",
    "                          for slice_ in output], dim=0)\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "class BipartiteNodeData(torch_geometric.data.Data):\n",
    "    def __init__(self, constraint_features, edge_indices, edge_features, variable_features,\n",
    "                 candidates, nb_candidates, candidate_choice, candidate_scores):\n",
    "        super().__init__()\n",
    "        self.constraint_features = constraint_features\n",
    "        self.edge_index = edge_indices\n",
    "        self.edge_attr = edge_features\n",
    "        self.variable_features = variable_features\n",
    "        self.candidates = candidates\n",
    "        self.nb_candidates = nb_candidates\n",
    "        self.candidate_choices = candidate_choice\n",
    "        self.candidate_scores = candidate_scores\n",
    "\n",
    "    def __inc__(self, key, value, store, *args, **kwargs):\n",
    "        if key == 'edge_index':\n",
    "            return torch.tensor([[self.constraint_features.size(0)], [self.variable_features.size(0)]])\n",
    "        elif key == 'candidates':\n",
    "            return self.variable_features.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "class GraphDataset(torch_geometric.data.Dataset):\n",
    "    def __init__(self, sample_files):\n",
    "        super().__init__(root=None, transform=None, pre_transform=None)\n",
    "        self.sample_files = sample_files\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.sample_files)\n",
    "\n",
    "    def get(self, index):\n",
    "        with gzip.open(self.sample_files[index], 'rb') as f:\n",
    "            sample = pickle.load(f)\n",
    "\n",
    "        sample_observation, sample_action, sample_action_set, sample_scores = sample['data']\n",
    "\n",
    "        constraint_features, (edge_indices, edge_features), variable_features = sample_observation\n",
    "        constraint_features = torch.FloatTensor(constraint_features)\n",
    "        edge_indices = torch.LongTensor(edge_indices.astype(np.int32))\n",
    "        edge_features = torch.FloatTensor(np.expand_dims(edge_features, axis=-1))\n",
    "        variable_features = torch.FloatTensor(variable_features)\n",
    "\n",
    "        candidates = torch.LongTensor(np.array(sample_action_set, dtype=np.int32))\n",
    "        candidate_choice = torch.where(candidates == sample_action)[0][0]  # action index relative to candidates\n",
    "        candidate_scores = torch.FloatTensor([sample_scores[j] for j in candidates])\n",
    "\n",
    "        graph = BipartiteNodeData(constraint_features, edge_indices, edge_features, variable_features,\n",
    "                                  candidates, len(candidates), candidate_choice, candidate_scores)\n",
    "        graph.num_nodes = constraint_features.shape[0]+variable_features.shape[0]\n",
    "        return graph\n",
    "\n",
    "\n",
    "\n",
    "class Scheduler(torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "    def __init__(self, optimizer, **kwargs):\n",
    "        super().__init__(optimizer, **kwargs)\n",
    "\n",
    "    def step(self, metrics):\n",
    "        # convert `metrics` to float, in case it's a zero-dim Tensor\n",
    "        current = float(metrics)\n",
    "        self.last_epoch =+1\n",
    "\n",
    "        if self.is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad_epochs = 0\n",
    "        else:\n",
    "            self.num_bad_epochs += 1\n",
    "\n",
    "        if self.num_bad_epochs == self.patience:\n",
    "            self._reduce_lr(self.last_epoch)\n",
    "\n",
    "        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous Models and Code References\n",
    "\n",
    "**Original GCN (2019)**  \n",
    "Based on the NeurIPS paper *Exact Combinatorial Optimization with Graph Convolutional Neural Networks*  \n",
    "(Gasse, M., Chételat, D., Ferroni, N., Charlin, L., & Lodi, A., 2019).  \n",
    "Code available at: [ds4dm/learn2branch](https://github.com/ds4dm/learn2branch)\n",
    "\n",
    "**Attention GCN (Attention Mechanism)**  \n",
    "*BiGNN: Bipartite Graph Neural Network with Attention Mechanism for Solving Multiple Traveling Salesman Problems in Urban Logistics*  \n",
    "(Liang, H., Wang, S., & Li, H., n.d.).  \n",
    "Code available at: [CO-RL/DeepMTSP](https://github.com/CO-RL/DeepMTSP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-xH1jO0P2hg"
   },
   "source": [
    "## MAGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 38,
     "status": "ok",
     "timestamp": 1747708806522,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "MD1-9InGP02y"
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MessagePassing\n",
    "import torch_geometric\n",
    "\n",
    "class PreNormException(Exception):\n",
    "    \"\"\"Exception to signal completion of PreNormLayer pre-training.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "class PreNormLayer(torch.nn.Module):\n",
    "    def __init__(self, n_units, shift=True, scale=True, name=None):\n",
    "        super().__init__()\n",
    "        assert shift or scale\n",
    "        self.register_buffer('shift', torch.zeros(n_units) if shift else None)\n",
    "        self.register_buffer('scale', torch.ones(n_units) if scale else None)\n",
    "        self.n_units = n_units\n",
    "        self.waiting_updates = False\n",
    "        self.received_updates = False\n",
    "\n",
    "    def forward(self, input_):\n",
    "        if self.waiting_updates:\n",
    "            self.update_stats(input_)\n",
    "            self.received_updates = True\n",
    "            raise PreNormException\n",
    "\n",
    "        if self.shift is not None:\n",
    "            input_ = input_ + self.shift\n",
    "\n",
    "        if self.scale is not None:\n",
    "            input_ = input_ * self.scale\n",
    "\n",
    "        return input_\n",
    "\n",
    "    def start_updates(self):\n",
    "        self.avg = 0\n",
    "        self.var = 0\n",
    "        self.m2 = 0\n",
    "        self.count = 0\n",
    "        self.waiting_updates = True\n",
    "        self.received_updates = False\n",
    "\n",
    "    def update_stats(self, input_):\n",
    "        \"\"\"\n",
    "        Online mean and variance estimation. See: Chan et al. (1979) Updating\n",
    "        Formulae and a Pairwise Algorithm for Computing Sample Variances.\n",
    "        https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm\n",
    "        \"\"\"\n",
    "        assert self.n_units == 1 or input_.shape[-1] == self.n_units, f\"Expected input dimension of size {self.n_units}, got {input_.shape[-1]}.\"\n",
    "\n",
    "        input_ = input_.reshape(-1, self.n_units)\n",
    "        sample_avg = input_.mean(dim=0)\n",
    "        sample_var = (input_ - sample_avg).pow(2).mean(dim=0)\n",
    "        sample_count = np.prod(input_.size())/self.n_units\n",
    "\n",
    "        delta = sample_avg - self.avg\n",
    "\n",
    "        self.m2 = self.var * self.count + sample_var * sample_count + delta ** 2 * self.count * sample_count / (\n",
    "                self.count + sample_count)\n",
    "\n",
    "        self.count += sample_count\n",
    "        self.avg += delta * sample_count / self.count\n",
    "        self.var = self.m2 / self.count if self.count > 0 else 1\n",
    "\n",
    "    def stop_updates(self):\n",
    "        \"\"\"\n",
    "        Ends pre-training for that layer, and fixes the layers's parameters.\n",
    "        \"\"\"\n",
    "        assert self.count > 0\n",
    "        if self.shift is not None:\n",
    "            self.shift = -self.avg\n",
    "\n",
    "        if self.scale is not None:\n",
    "            self.var[self.var < 1e-8] = 1\n",
    "            self.scale = 1 / torch.sqrt(self.var)\n",
    "\n",
    "        del self.avg, self.var, self.m2, self.count\n",
    "        self.waiting_updates = False\n",
    "        self.trainable = False\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# Aggregation Functions\n",
    "##############################################################################\n",
    "def sum_aggregate(inputs, index, dim_size=None):\n",
    "    \"\"\"\n",
    "    Sum aggregator using torch.index_add_.\n",
    "    inputs : [E, D]  (E edges, D embedding dim)\n",
    "    index  : [E]     (which node each edge belongs to)\n",
    "    \"\"\"\n",
    "    if dim_size is None:\n",
    "        dim_size = int(index.max()) + 1\n",
    "    out = torch.zeros(dim_size, inputs.size(1), device=inputs.device)\n",
    "    out.index_add_(0, index, inputs)\n",
    "    return out\n",
    "\n",
    "\n",
    "def mean_aggregate(inputs, index, dim_size=None):\n",
    "    \"\"\"\n",
    "    Mean aggregator using sum_aggregate and torch.bincount.\n",
    "    \"\"\"\n",
    "    if dim_size is None:\n",
    "        dim_size = int(index.max()) + 1\n",
    "    sums = sum_aggregate(inputs, index, dim_size)\n",
    "    counts = torch.bincount(index, minlength=dim_size).float().unsqueeze(1)\n",
    "    counts = counts.clamp_min(1)  # Prevent division by zero\n",
    "    mean = sums / counts\n",
    "    return mean\n",
    "\n",
    "\n",
    "def max_aggregate(inputs, index, dim_size=None):\n",
    "    \"\"\"\n",
    "    Max aggregator using torch.scatter_reduce.\n",
    "    \"\"\"\n",
    "    if dim_size is None:\n",
    "        dim_size = int(index.max()) + 1\n",
    "    # Initialize with very small values\n",
    "    out = torch.full((dim_size, inputs.size(1)), float('-inf'), device=inputs.device)\n",
    "    # Perform scatter reduce with 'amax' (max) operation\n",
    "    out = out.scatter_reduce(0, index.unsqueeze(-1).expand_as(inputs), inputs, reduce='amax', include_self=True)\n",
    "    return out\n",
    "\n",
    "def aggregate_var(inputs, index, dim_size=None):\n",
    "    \"\"\"\n",
    "    Computes the variance of the neighboring node features.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Input features [E, D], where E is the number of edges.\n",
    "        index (torch.Tensor): Index tensor [E], indicating the target node for each edge.\n",
    "        dim_size (int, optional): Total number of nodes. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Variance aggregated features [N, D], where N is the number of nodes.\n",
    "    \"\"\"\n",
    "    if dim_size is None:\n",
    "        dim_size = int(index.max()) + 1\n",
    "\n",
    "    # Compute D^{-1} A X^2\n",
    "    X_squared = inputs.pow(2)  # [E, D]\n",
    "    mean_agg_X2 = mean_aggregate(X_squared, index, dim_size)  # [N, D]\n",
    "\n",
    "    # Compute (D^{-1} A X)^2\n",
    "    mean_agg = mean_aggregate(inputs, index, dim_size)  # [N, D]\n",
    "    X_mean_sq = mean_agg.pow(2)  # [N, D]\n",
    "\n",
    "    # Compute variance: ReLU(D^{-1} A X^2 - (D^{-1} A X)^2)\n",
    "    var = F.relu(mean_agg_X2 - X_mean_sq)  # [N, D]\n",
    "\n",
    "    return var\n",
    "\n",
    "def aggregate_std(inputs, index, dim_size=None):\n",
    "    \"\"\"\n",
    "    Computes the standard deviation of the neighboring node features.\n",
    "\n",
    "    Args:\n",
    "        inputs (torch.Tensor): Input features [E, D], where E is the number of edges.\n",
    "        index (torch.Tensor): Index tensor [E], indicating the target node for each edge.\n",
    "        dim_size (int, optional): Total number of nodes. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Standard deviation aggregated features [N, D], where N is the number of nodes.\n",
    "    \"\"\"\n",
    "    var = aggregate_var(inputs, index, dim_size)  # [N, D]\n",
    "    std = torch.sqrt(var + 1e-8)  # [N, D]\n",
    "    return std\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################\n",
    "# MultiAggregatorBipartiteConvManual with PreNormLayers\n",
    "##############################################################################\n",
    "class MultiAggregatorBipartiteConvManual_PNA(MessagePassing):\n",
    "    def __init__(self, emb_size=64, avg_d=1.0):\n",
    "        super().__init__(aggr=None)  # We'll manually aggregate\n",
    "        self.emb_size = emb_size\n",
    "        self.avg_d = avg_d\n",
    "        # Linear transforms for node/edge embeddings\n",
    "        self.linear_left = nn.Linear(emb_size, emb_size, bias=False)\n",
    "        self.linear_right = nn.Linear(emb_size, emb_size, bias=False)\n",
    "        self.linear_edge = nn.Linear(1, emb_size, bias=False)\n",
    "\n",
    "        # PreNormLayer for feature_module_final\n",
    "        self.feature_module_final = nn.Sequential(\n",
    "            PreNormLayer(1, shift=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "\n",
    "        # PreNormLayer for post_conv_module\n",
    "        self.post_conv_module = nn.Sequential(\n",
    "            PreNormLayer(1, shift=False)\n",
    "        )\n",
    "\n",
    "        # self.highway_mlp = nn.Sequential(\n",
    "        #     nn.Linear(4 * emb_size, emb_size),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(0.1),\n",
    "        #     nn.Linear(emb_size, emb_size)\n",
    "        # )\n",
    "\n",
    "\n",
    "        self.out_mlp = nn.Sequential(\n",
    "            nn.Linear(12 * emb_size, 3 * emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(3 * emb_size, emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "\n",
    "        # Output module (newly added for concatenation)\n",
    "        self.output_module = nn.Sequential(\n",
    "            nn.Linear(2 * emb_size, emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, left_feats, edge_index, edge_feats, right_feats):\n",
    "        \"\"\"\n",
    "        left_feats : [N_left, emb_size]\n",
    "        right_feats: [N_right, emb_size]\n",
    "        edge_feats : [E, 1]\n",
    "        edge_index : [2, E]\n",
    "        \"\"\"\n",
    "        # Apply feature_module_final PreNormLayer\n",
    "        # processed_right_feats = self.feature_module_final(right_feats)\n",
    "\n",
    "        # Perform message passing\n",
    "        aggr_out = self.propagate(\n",
    "            edge_index=edge_index,\n",
    "            size=(left_feats.size(0), right_feats.size(0)),\n",
    "            node_features=(left_feats, right_feats),\n",
    "            edge_features=edge_feats\n",
    "        )\n",
    "\n",
    "        # Concatenate with original right_feats\n",
    "        combined = torch.cat([self.post_conv_module(aggr_out), right_feats], dim=-1)\n",
    "\n",
    "        # Pass through output_module\n",
    "        return self.output_module(combined)\n",
    "\n",
    "    def message(self, node_features_i, node_features_j, edge_features):\n",
    "        \"\"\"\n",
    "        node_features_i, node_features_j: [E, emb_size]\n",
    "        edge_features: [E, 1]\n",
    "        \"\"\"\n",
    "        fi = self.linear_left(node_features_i)\n",
    "        fj = self.linear_right(node_features_j)\n",
    "        fe = self.linear_edge(edge_features)\n",
    "\n",
    "        # Sum the transformed features\n",
    "        combined = fi + fj + fe\n",
    "\n",
    "        # Apply feature_module_final within message\n",
    "        output = self.feature_module_final(combined)  # Normalize and transform per message\n",
    "\n",
    "        return output\n",
    "\n",
    "    def aggregate(self, inputs, index, dim_size=None):\n",
    "\n",
    "\n",
    "        if dim_size is None:\n",
    "            dim_size = int(index.max()) + 1\n",
    "\n",
    "        device = inputs.device\n",
    "\n",
    "        # Compute node degrees\n",
    "        degrees = torch.bincount(index, minlength=dim_size).float().unsqueeze(1)  # [N_nodes, 1]\n",
    "        degrees = degrees.clamp_min(1.0)  # Prevent division by zero\n",
    "\n",
    "        # Perform aggregations\n",
    "        mean_agg = mean_aggregate(inputs, index, dim_size)    # [N_nodes, emb_size]\n",
    "        sum_agg = sum_aggregate(inputs, index, dim_size)      # [N_nodes, emb_size]\n",
    "        max_agg = max_aggregate(inputs, index, dim_size)      # [N_nodes, emb_size]\n",
    "        # var_agg = aggregate_var(inputs, index, dim_size)  # [N_nodes, emb_size]\n",
    "        std_agg = aggregate_std(inputs, index, dim_size)  # [N_nodes, emb_size]\n",
    "\n",
    "        # Apply degree scalers: Identity, Amplification, Attenuation\n",
    "        # For Mean Aggregation\n",
    "        scaled_mean_identity = mean_agg  # Identity\n",
    "        scaled_mean_amplification = (torch.log(degrees + 1) / self.avg_d) * mean_agg  # Amplification\n",
    "        scaled_mean_attenuation = (self.avg_d / torch.log(degrees + 1)) * mean_agg    # Attenuation\n",
    "\n",
    "        # For Sum Aggregation\n",
    "        scaled_sum_identity = sum_agg  # Identity\n",
    "        scaled_sum_amplification = (torch.log(degrees + 1) / self.avg_d) * sum_agg    # Amplification\n",
    "        scaled_sum_attenuation = (self.avg_d / torch.log(degrees + 1)) * sum_agg      # Attenuation\n",
    "\n",
    "        # For Max Aggregation\n",
    "        scaled_max_identity = max_agg  # Identity\n",
    "        scaled_max_amplification = (torch.log(degrees + 1) / self.avg_d) * max_agg    # Amplification\n",
    "        scaled_max_attenuation = (self.avg_d / torch.log(degrees + 1)) * max_agg      # Attenuation\n",
    "\n",
    "        scaled_std_identity = std_agg  # Identity\n",
    "        scaled_std_amplification = (torch.log(degrees + 1) / self.avg_d) * std_agg  # Amplification\n",
    "        scaled_std_attenuation = (self.avg_d / torch.log(degrees + 1)) * std_agg    # Attenuation\n",
    "\n",
    "        # Identity aggregations processed through highway MLP\n",
    "        identity_agg = torch.cat([\n",
    "            scaled_mean_identity,\n",
    "            scaled_sum_identity,\n",
    "            scaled_max_identity,\n",
    "            scaled_std_identity\n",
    "        ], dim=-1)\n",
    "\n",
    "        # Concatenate identity aggregations for Out MLP\n",
    "        cat_agg_out = torch.cat([\n",
    "            scaled_mean_identity,\n",
    "            scaled_sum_identity,\n",
    "            scaled_max_identity,\n",
    "            scaled_std_identity,\n",
    "            scaled_mean_amplification,\n",
    "            scaled_sum_amplification,\n",
    "            scaled_max_amplification,\n",
    "            scaled_std_amplification,\n",
    "            scaled_mean_attenuation,\n",
    "            scaled_sum_attenuation,\n",
    "            scaled_max_attenuation,\n",
    "            scaled_std_attenuation\n",
    "        ], dim=-1)  # [N_nodes, 3 * emb_size]\n",
    "\n",
    "        # # Pass through gate_mlp or further processing as needed\n",
    "        # gating_logits = self.gate_mlp(cat_agg_gate)               # [N_nodes, num_gates]\n",
    "        # gating_weights = F.softmax(gating_logits, dim=-1)    # [N_nodes, num_gates]\n",
    "\n",
    "        # # Weighted sum across [mean, max, sum]\n",
    "        # stacked = torch.stack([mean_agg, max_agg, sum_agg], dim=1)  # [N_nodes, 3, emb_size]\n",
    "        # fused = (stacked * gating_weights.unsqueeze(-1)).sum(dim=1)  # [N_nodes, emb_size]\n",
    "\n",
    "        # skip_output = self.highway_mlp(identity_agg)\n",
    "\n",
    "\n",
    "        out = self.out_mlp(cat_agg_out)  # [N_nodes, emb_size]\n",
    "\n",
    "        # Add skip_output to out to get mid_output\n",
    "        mid_output = out + scaled_sum_identity  # [N_nodes, emb_size]\n",
    "\n",
    "        # Combine them (residual style)\n",
    "        # combined = out  # [N_nodes, emb_size]\n",
    "\n",
    "        # Apply post_conv_module PreNormLayer\n",
    "        combined = self.post_conv_module(mid_output)\n",
    "\n",
    "        return combined\n",
    "\n",
    "    def update(self, aggr_out):\n",
    "        return aggr_out\n",
    "\n",
    "#############################################################################\n",
    "## BaseModel\n",
    "#############################################################################\n",
    "class BaseModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Our base model class, which implements pre-training methods.\n",
    "    \"\"\"\n",
    "    def pre_train_init(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, PreNormLayer):\n",
    "                module.start_updates()\n",
    "\n",
    "    def pre_train_next(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, PreNormLayer) and module.waiting_updates and module.received_updates:\n",
    "                module.stop_updates()\n",
    "                return module\n",
    "        return None\n",
    "\n",
    "    def pre_train(self, *args, **kwargs):\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                self.forward(*args, **kwargs)\n",
    "            return False\n",
    "        except PreNormException:\n",
    "            return True\n",
    "\n",
    "\n",
    "class GNNPolicy(BaseModel):\n",
    "    def __init__(self, cons_nfeats=5, var_nfeats=19, edge_nfeats=1,\n",
    "                 emb_size=64, avg_d_left=1.0, avg_d_right=1.0):\n",
    "        super().__init__()\n",
    "        self.avg_d_left = avg_d_left\n",
    "        self.avg_d_right = avg_d_right\n",
    "\n",
    "        # Constraint embedding\n",
    "        self.cons_embedding = nn.Sequential(\n",
    "            PreNormLayer(cons_nfeats),\n",
    "            nn.Linear(cons_nfeats, emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Variable embedding\n",
    "        self.var_embedding = nn.Sequential(\n",
    "            PreNormLayer(var_nfeats),\n",
    "            nn.Linear(var_nfeats, emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Edge embedding\n",
    "        self.edge_embedding = nn.Sequential(\n",
    "            PreNormLayer(edge_nfeats)\n",
    "        )\n",
    "\n",
    "        self.v_to_c_layer = MultiAggregatorBipartiteConvManual_PNA(\n",
    "            emb_size=emb_size,\n",
    "            avg_d=self.avg_d_left\n",
    "        )\n",
    "        self.c_to_v_layer = MultiAggregatorBipartiteConvManual_PNA(\n",
    "            emb_size=emb_size,\n",
    "            avg_d=self.avg_d_right\n",
    "        )\n",
    "\n",
    "\n",
    "        # Self-attention layers (Q, K, V)\n",
    "        self.Q = torch.nn.Linear(emb_size, emb_size, bias=True)\n",
    "        self.K = torch.nn.Linear(emb_size, emb_size, bias=True)\n",
    "        self.V = torch.nn.Linear(emb_size, emb_size, bias=True)\n",
    "\n",
    "        # Output head with PreNormLayers if needed (optional)\n",
    "        self.output_module = nn.Sequential(\n",
    "            nn.Linear(emb_size, emb_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(emb_size, 1, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, constraint_feats, edge_index, edge_feats, variable_feats):\n",
    "        \"\"\"\n",
    "        constraint_feats : [N_constraints, cons_nfeats]\n",
    "        variable_feats   : [N_vars, var_nfeats]\n",
    "        edge_feats       : [E, edge_nfeats]\n",
    "        edge_index       : [2, E] => [constraint_idx, variable_idx]\n",
    "        \"\"\"\n",
    "        # 1) Initial embeddings\n",
    "        c_emb = self.cons_embedding(constraint_feats)  # [N_constraints, emb_size]\n",
    "        v_emb = self.var_embedding(variable_feats)      # [N_vars, emb_size]\n",
    "        e_emb = self.edge_embedding(edge_feats)         # [E, 1]\n",
    "\n",
    "        # 2) Reverse edge index for var->cons message passing\n",
    "        rev_edge_index = torch.stack([edge_index[1], edge_index[0]], dim=0)\n",
    "\n",
    "        # 3) Single bipartite pass\n",
    "        #    (a) variable -> constraint\n",
    "        c_new = self.v_to_c_layer(\n",
    "            left_feats=v_emb,\n",
    "            edge_index=rev_edge_index,\n",
    "            edge_feats=e_emb,\n",
    "            right_feats=c_emb\n",
    "        )\n",
    "\n",
    "        #    (b) constraint -> variable\n",
    "        variable_features = self.c_to_v_layer(\n",
    "            left_feats=c_emb,\n",
    "            edge_index=edge_index,\n",
    "            edge_feats=e_emb,\n",
    "            right_feats=v_emb\n",
    "        )\n",
    "\n",
    "\n",
    "        # Step 3: Self-attention block (Exact replication of TensorFlow approach)\n",
    "        Q = self.Q(variable_features)    # [num_vars, emb_size]\n",
    "        K = self.K(variable_features)    # [num_vars, emb_size]\n",
    "        V = self.V(variable_features)    # [num_vars, emb_size]\n",
    "\n",
    "        # Compute attention scores: [emb_size, emb_size]\n",
    "        # Equivalent to tf.matmul(tf.transpose(Q), K)/8\n",
    "        attention_scores = torch.matmul(Q.transpose(0, 1), K) / 8.0  # [emb_size, emb_size]\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # [emb_size, emb_size]\n",
    "\n",
    "        # Compute the weighted sum of values: [num_vars, emb_size]\n",
    "        # Equivalent to tf.matmul(V, attention)\n",
    "        attended_features = torch.matmul(V, attention_weights)  # [num_vars, emb_size]\n",
    "\n",
    "        # Apply activation\n",
    "        attended_features = F.relu(attended_features)\n",
    "\n",
    "        # Step 4: Final output\n",
    "        output = self.output_module(attended_features).squeeze(-1)  # [num_vars]\n",
    "\n",
    "\n",
    "        # # 4) Score each variable node\n",
    "        # logits = self.output_module(v_new).squeeze(-1)  # [N_vars]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GOx9amDLuPXi"
   },
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1747708808571,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "xltqJGAHtZIW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import gzip\n",
    "import argparse\n",
    "import pickle\n",
    "import queue\n",
    "import shutil\n",
    "import threading\n",
    "import numpy as np\n",
    "import ecole\n",
    "from collections import namedtuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747708808946,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "lS4ZoPN4uOiC"
   },
   "outputs": [],
   "source": [
    "class ExploreThenStrongBranch:\n",
    "    def __init__(self, expert_probability):\n",
    "        self.expert_probability = expert_probability\n",
    "        self.pseudocosts_function = ecole.observation.Pseudocosts()\n",
    "        self.strong_branching_function = ecole.observation.StrongBranchingScores()\n",
    "\n",
    "    def before_reset(self, model):\n",
    "        self.pseudocosts_function.before_reset(model)\n",
    "        self.strong_branching_function.before_reset(model)\n",
    "\n",
    "    def extract(self, model, done):\n",
    "        probabilities = [1-self.expert_probability, self.expert_probability]\n",
    "        expert_chosen = bool(np.random.choice(np.arange(2), p=probabilities))\n",
    "        if expert_chosen:\n",
    "            return (self.strong_branching_function.extract(model,done), True)\n",
    "        else:\n",
    "            return (self.pseudocosts_function.extract(model,done), False)\n",
    "\n",
    "\n",
    "def send_orders(orders_queue, instances, seed, query_expert_prob, time_limit, out_dir, stop_flag):\n",
    "    \"\"\"\n",
    "    Continuously send sampling orders to workers (relies on limited\n",
    "    queue capacity).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    orders_queue : queue.Queue\n",
    "        Queue to which to send orders.\n",
    "    instances : list\n",
    "        Instance file names from which to sample episodes.\n",
    "    seed : int\n",
    "        Random seed for reproducibility.\n",
    "    query_expert_prob : float in [0, 1]\n",
    "        Probability of running the expert strategy and collecting samples.\n",
    "    time_limit : float in [0, 1e+20]\n",
    "        Maximum running time for an episode, in seconds.\n",
    "    out_dir: str\n",
    "        Output directory in which to write samples.\n",
    "    stop_flag: threading.Event\n",
    "        A flag to tell the thread to stop.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    episode = 0\n",
    "    while not stop_flag.is_set():\n",
    "        instance = rng.choice(instances)\n",
    "        seed = rng.randint(2**32)\n",
    "        orders_queue.put([episode, instance, seed, query_expert_prob, time_limit, out_dir])\n",
    "        episode += 1\n",
    "\n",
    "\n",
    "def make_samples(in_queue, out_queue, stop_flag):\n",
    "    \"\"\"\n",
    "    Worker loop: fetch an instance, run an episode and record samples.\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_queue : queue.Queue\n",
    "        Input queue from which orders are received.\n",
    "    out_queue : queue.Queue\n",
    "        Output queue in which to send samples.\n",
    "    stop_flag: threading.Event\n",
    "        A flag to tell the thread to stop.\n",
    "    \"\"\"\n",
    "    sample_counter = 0\n",
    "    while not stop_flag.is_set():\n",
    "        episode, instance, seed, query_expert_prob, time_limit, out_dir = in_queue.get()\n",
    "\n",
    "        scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0,\n",
    "                           'limits/time': time_limit, 'timing/clocktype': 2}\n",
    "        observation_function = { \"scores\": ExploreThenStrongBranch(expert_probability=query_expert_prob),\n",
    "                                 \"node_observation\": ecole.observation.NodeBipartite() }\n",
    "        env = ecole.environment.Branching(observation_function=observation_function,\n",
    "                                          scip_params=scip_parameters, pseudo_candidates=True)\n",
    "\n",
    "        print(f\"[w {threading.current_thread().name}] episode {episode}, seed {seed}, \"\n",
    "              f\"processing instance '{instance}'...\\n\", end='')\n",
    "        out_queue.put({\n",
    "            'type': 'start',\n",
    "            'episode': episode,\n",
    "            'instance': instance,\n",
    "            'seed': seed,\n",
    "        })\n",
    "\n",
    "        env.seed(seed)\n",
    "        observation, action_set, _, done, _ = env.reset(instance)\n",
    "        while not done:\n",
    "            scores, scores_are_expert = observation[\"scores\"]\n",
    "            node_observation = observation[\"node_observation\"]\n",
    "            node_observation = (node_observation.row_features,\n",
    "                                (node_observation.edge_features.indices,\n",
    "                                 node_observation.edge_features.values),\n",
    "                                node_observation.variable_features)\n",
    "\n",
    "            action = action_set[scores[action_set].argmax()]\n",
    "\n",
    "            if scores_are_expert and not stop_flag.is_set():\n",
    "                data = [node_observation, action, action_set, scores]\n",
    "                filename = f'{out_dir}/sample_{episode}_{sample_counter}.pkl'\n",
    "\n",
    "                with gzip.open(filename, 'wb') as f:\n",
    "                    pickle.dump({\n",
    "                        'episode': episode,\n",
    "                        'instance': instance,\n",
    "                        'seed': seed,\n",
    "                        'data': data,\n",
    "                        }, f)\n",
    "                out_queue.put({\n",
    "                    'type': 'sample',\n",
    "                    'episode': episode,\n",
    "                    'instance': instance,\n",
    "                    'seed': seed,\n",
    "                    'filename': filename,\n",
    "                })\n",
    "                sample_counter += 1\n",
    "\n",
    "            try:\n",
    "                observation, action_set, _, done, _ = env.step(action)\n",
    "            except Exception as e:\n",
    "                done = True\n",
    "                with open(\"error_log.txt\",\"a\") as f:\n",
    "                    f.write(f\"Error occurred solving {instance} with seed {seed}\\n\")\n",
    "                    f.write(f\"{e}\\n\")\n",
    "\n",
    "        print(f\"[w {threading.current_thread().name}] episode {episode} done, {sample_counter} samples\\n\", end='')\n",
    "        out_queue.put({\n",
    "            'type': 'done',\n",
    "            'episode': episode,\n",
    "            'instance': instance,\n",
    "            'seed': seed,\n",
    "        })\n",
    "\n",
    "\n",
    "import os\n",
    "import queue\n",
    "import threading\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "# def get_last_sample_index(out_dir):\n",
    "#     \"\"\"\n",
    "#     Scans the output directory for existing samples and finds the highest index.\n",
    "#     \"\"\"\n",
    "#     sample_files = [f for f in os.listdir(out_dir) if re.match(r'sample_\\d+\\.pkl', f)]\n",
    "#     if sample_files:\n",
    "#         # Extract numbers from the filenames and find the maximum\n",
    "#         indices = [int(re.findall(r'\\d+', f)[0]) for f in sample_files]\n",
    "#         return max(indices)\n",
    "#     else:\n",
    "#         return 0\n",
    "\n",
    "def collect_samples(instances, out_dir, rng, n_samples, n_jobs,\n",
    "                    query_expert_prob, time_limit, last_index):\n",
    "    \"\"\"\n",
    "    Runs branch-and-bound episodes on the given set of instances, and collects\n",
    "    randomly (state, action) pairs from the 'vanilla-fullstrong' expert\n",
    "    brancher.\n",
    "    Parameters\n",
    "    ----------\n",
    "    instances : list\n",
    "        Instance files from which to collect samples.\n",
    "    out_dir : str\n",
    "        Directory in which to write samples.\n",
    "    rng : numpy.random.RandomState\n",
    "        A random number generator for reproducibility.\n",
    "    n_samples : int\n",
    "        Number of samples to collect.\n",
    "    n_jobs : int\n",
    "        Number of jobs for parallel sampling.\n",
    "    query_expert_prob : float in [0, 1]\n",
    "        Probability of using the expert policy and recording a (state, action)\n",
    "        pair.\n",
    "    time_limit : float in [0, 1e+20]\n",
    "        Maximum running time for an episode, in seconds.\n",
    "    \"\"\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Start index from the last sample file in the directory\n",
    "    #last_index = 2000\n",
    "    print(f\"Starting from sample index {last_index + 1}\")\n",
    "\n",
    "    # start workers\n",
    "    orders_queue = queue.Queue(maxsize=2*n_jobs)\n",
    "    answers_queue = queue.SimpleQueue()\n",
    "\n",
    "    tmp_samples_dir = f'{out_dir}/tmp'\n",
    "    os.makedirs(tmp_samples_dir, exist_ok=True)\n",
    "\n",
    "    # start dispatcher\n",
    "    dispatcher_stop_flag = threading.Event()\n",
    "    dispatcher = threading.Thread(\n",
    "            target=send_orders,\n",
    "            args=(orders_queue, instances, rng.randint(2**32), query_expert_prob,\n",
    "                  time_limit, tmp_samples_dir, dispatcher_stop_flag),\n",
    "            daemon=True)\n",
    "    dispatcher.start()\n",
    "\n",
    "    workers = []\n",
    "    workers_stop_flag = threading.Event()\n",
    "    for i in range(n_jobs):\n",
    "        p = threading.Thread(\n",
    "                target=make_samples,\n",
    "                args=(orders_queue, answers_queue, workers_stop_flag),\n",
    "                daemon=True)\n",
    "        workers.append(p)\n",
    "        p.start()\n",
    "\n",
    "    # record answers and write samples\n",
    "    buffer = {}\n",
    "    current_episode = 0\n",
    "    i = 0\n",
    "    in_buffer = 0\n",
    "    while i < n_samples:\n",
    "        sample = answers_queue.get()\n",
    "\n",
    "        # add received sample to buffer\n",
    "        if sample['type'] == 'start':\n",
    "            buffer[sample['episode']] = []\n",
    "        else:\n",
    "            buffer[sample['episode']].append(sample)\n",
    "            if sample['type'] == 'sample':\n",
    "                in_buffer += 1\n",
    "\n",
    "        # if any, write samples from current episode\n",
    "        while current_episode in buffer and buffer[current_episode]:\n",
    "            samples_to_write = buffer[current_episode]\n",
    "            buffer[current_episode] = []\n",
    "\n",
    "            for sample in samples_to_write:\n",
    "\n",
    "                # if no more samples here, move to next episode\n",
    "                if sample['type'] == 'done':\n",
    "                    del buffer[current_episode]\n",
    "                    current_episode += 1\n",
    "\n",
    "                # else write sample\n",
    "                else:\n",
    "                    sample_index = last_index + i + 1\n",
    "                    os.rename(sample['filename'], f'{out_dir}/sample_{sample_index}.pkl')\n",
    "                    in_buffer -= 1\n",
    "                    i += 1\n",
    "                    print(f\"[m {threading.current_thread().name}] {i} / {n_samples} samples written, \"\n",
    "                          f\"ep {sample['episode']} ({in_buffer} in buffer).\\n\", end='')\n",
    "\n",
    "                    # early stop dispatcher\n",
    "                    if in_buffer + i >= n_samples and dispatcher.is_alive():\n",
    "                        dispatcher_stop_flag.set()\n",
    "                        print(f\"[m {threading.current_thread().name}] dispatcher stopped...\\n\", end='')\n",
    "\n",
    "                    # as soon as enough samples are collected, stop\n",
    "                    if i == n_samples:\n",
    "                        buffer = {}\n",
    "                        break\n",
    "\n",
    "    # stop all workers\n",
    "    workers_stop_flag.set()\n",
    "    for p in workers:\n",
    "        p.join(timeout=15)  # Wait up to 5 seconds for each thread to terminate\n",
    "        if p.is_alive():\n",
    "            print(f\"Thread {p.name} is still running, skipping.\")\n",
    "\n",
    "    print(f\"Done collecting samples for {out_dir}\")\n",
    "    shutil.rmtree(tmp_samples_dir, ignore_errors=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 583349,
     "status": "ok",
     "timestamp": 1742660742243,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "m5YGhR3Duf9O",
    "outputId": "7efa9752-6c14-4c83-8020-b096d04fb831"
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'problem',\n",
    "        help='MILP instance type to process.',\n",
    "        # choices=['setcover', 'cauctions', 'facilities', 'indset', 'mknapsack'],\n",
    "        choices=['Standard_MTSP', 'MinMax_MTSP', 'Bounded_MTSP', 'JSSP'],\n",
    "\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-s', '--seed',\n",
    "        help='Random generator seed.',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-j', '--njobs',\n",
    "        help='Number of parallel jobs.',\n",
    "        type=int,\n",
    "        default=os.cpu_count(),\n",
    "    )\n",
    "    args = parser.parse_args(['JSSP'])\n",
    "\n",
    "    print(f\"seed {args.seed}\")\n",
    "\n",
    "    train_size = 10000\n",
    "    valid_size = 4000\n",
    "    node_record_prob = 0.05\n",
    "    time_limit = 4800\n",
    "\n",
    "    if args.problem == 'setcover':\n",
    "        instances_train = glob.glob('data/instances/setcover/train_500r_1000c_0.05d/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/setcover/valid_500r_1000c_0.05d/*.lp')\n",
    "        instances_test = glob.glob('data/instances/setcover/test_500r_1000c_0.05d/*.lp')\n",
    "        out_dir = 'data/samples/setcover/500r_1000c_0.05d'\n",
    "\n",
    "    elif args.problem == 'cauctions':\n",
    "        instances_train = glob.glob('data/instances/cauctions/train_100_500/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/cauctions/valid_100_500/*.lp')\n",
    "        instances_test = glob.glob('data/instances/cauctions/test_100_500/*.lp')\n",
    "        out_dir = 'data/samples/cauctions/100_500'\n",
    "\n",
    "    elif args.problem == 'indset':\n",
    "        instances_train = glob.glob('data/instances/indset/train_500_4/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/indset/valid_500_4/*.lp')\n",
    "        instances_test = glob.glob('data/instances/indset/test_500_4/*.lp')\n",
    "        out_dir = 'data/samples/indset/500_4'\n",
    "\n",
    "    elif args.problem == 'facilities':\n",
    "        instances_train = glob.glob('data/instances/facilities/train_100_100_5/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/facilities/valid_100_100_5/*.lp')\n",
    "        instances_test = glob.glob('data/instances/facilities/test_100_100_5/*.lp')\n",
    "        out_dir = 'data/samples/facilities/100_100_5'\n",
    "        time_limit = 4800\n",
    "\n",
    "    elif args.problem == 'mknapsack':\n",
    "        instances_train = glob.glob('data/instances/mknapsack/train_100_6/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/mknapsack/valid_100_6/*.lp')\n",
    "        instances_test = glob.glob('data/instances/mknapsack/test_100_6/*.lp')\n",
    "        out_dir = 'data/samples/mknapsack/100_6'\n",
    "        time_limit = 4800\n",
    "\n",
    "    elif args.problem == 'Standard_MTSP':\n",
    "        instances_train = glob.glob('data/instances/Standard_MTSP/train_12_3/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/Standard_MTSP/valid_12_3/*.lp')\n",
    "        # instances_test = glob.glob('data/instances/Standard_MTSP/test_12_3/*.lp')\n",
    "        out_dir = 'data/samples/Standard_MTSP/12_3'\n",
    "        time_limit = 4800\n",
    "\n",
    "    elif args.problem == 'Bounded_MTSP':\n",
    "        instances_train = glob.glob('data/instances/Bounded_MTSP/train_12_3/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/Bounded_MTSP/valid_12_3/*.lp')\n",
    "        # instances_test = glob.glob('data/instances/Bounded_MTSP/test_12_3/*.lp')\n",
    "        out_dir = 'data/samples/Bounded_MTSP/12_3'\n",
    "        time_limit = 4800\n",
    "\n",
    "    elif args.problem == 'MinMax_MTSP':\n",
    "        instances_train = glob.glob('data/instances/MinMax_MTSP/train_9_3/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/MinMax_MTSP/valid_9_3/*.lp')\n",
    "        # instances_test = glob.glob('data/instances/MinMax_MTSP/test_12_3/*.lp')\n",
    "        out_dir = 'data/samples/MinMax_MTSP/9_3'\n",
    "        time_limit = 4800\n",
    "\n",
    "    elif args.problem == 'JSSP':\n",
    "        instances_train = glob.glob('data/instances/JSSP/train_6_3/*.lp')\n",
    "        instances_valid = glob.glob('data/instances/JSSP/valid_6_3/*.lp')\n",
    "        instances_test = glob.glob('data/instances/JSSP/test_6_3/*.lp')\n",
    "        out_dir = 'data/samples/JSSP/6_3'\n",
    "        time_limit = 4800\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    print(f\"{len(instances_train)} train instances for {train_size} samples\")\n",
    "    print(f\"{len(instances_valid)} validation instances for {valid_size} samples\")\n",
    "    # print(f\"{len(instances_test)} test instances for {test_size} samples\")\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    rng = np.random.RandomState(args.seed + 3)\n",
    "    collect_samples(instances_train, out_dir + '/train/1', rng, train_size,\n",
    "                    args.njobs, query_expert_prob=node_record_prob,\n",
    "                    time_limit=time_limit, last_index = 0)\n",
    "\n",
    "    rng = np.random.RandomState(args.seed + 4)\n",
    "    collect_samples(instances_train, out_dir + '/train/2', rng, train_size,\n",
    "                    args.njobs, query_expert_prob=node_record_prob,\n",
    "                    time_limit=time_limit, last_index = 0)\n",
    "\n",
    "    rng = np.random.RandomState(args.seed + 1)\n",
    "    collect_samples(instances_valid, out_dir + '/valid', rng, valid_size,\n",
    "                    args.njobs, query_expert_prob=node_record_prob,\n",
    "                    time_limit=time_limit, last_index = 0)\n",
    "\n",
    "    # rng = np.random.RandomState(args.seed + 2)\n",
    "    # collect_samples(instances_test, out_dir + '/test', rng, test_size,\n",
    "    #                 args.njobs, query_expert_prob=node_record_prob,\n",
    "    #                 time_limit=time_limit, last_index = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9L18Adnuw1r"
   },
   "source": [
    "# Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1747708814809,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "z32aAlInutIW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1747708815121,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "v7RtC6Ut8cZo",
    "outputId": "b3b6d210-3ab6-4b9f-d304-d29284b30a5c"
   },
   "outputs": [],
   "source": [
    "args = argparse.Namespace(gpu=0)  # Set to -1 for CPU, or specify GPU ID\n",
    "\n",
    "def set_device(gpu):\n",
    "    if gpu == -1 or not torch.cuda.is_available():\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "        return torch.device(\"cpu\")  # Use CPU\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = f'{gpu}'\n",
    "        return torch.device(\"cuda:0\")  # Use the specified GPU\n",
    "\n",
    "device = set_device(args.gpu)\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1747708815569,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "aT4BYCSkZFpk"
   },
   "outputs": [],
   "source": [
    "# Define the compute_avg_d_separated function\n",
    "def compute_avg_d_separated(loader, device=device):\n",
    "    \"\"\"\n",
    "    Computes the average of log(degree + 1) separately for left and right nodes\n",
    "    in bipartite graphs within the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - loader (torch_geometric.loader.DataLoader): DataLoader for the training dataset.\n",
    "    - device (torch.device): Device to perform computations on.\n",
    "\n",
    "    Returns:\n",
    "    - avg_log_left (float): Average of log(degree + 1) for left nodes.\n",
    "    - avg_log_right (float): Average of log(degree + 1) for right nodes.\n",
    "    \"\"\"\n",
    "    log_d_sum_left = 0.0\n",
    "    log_d_sum_right = 0.0\n",
    "    left_count = 0\n",
    "    right_count = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "\n",
    "        # Number of left and right nodes\n",
    "        num_left = data.constraint_features.size(0)\n",
    "        num_right = data.variable_features.size(0)\n",
    "\n",
    "        # Compute degrees for left nodes (sources in edge_index[0])\n",
    "        left_degrees = torch.bincount(\n",
    "            data.edge_index[0],\n",
    "            minlength=num_left\n",
    "        ).float()\n",
    "\n",
    "        # Compute degrees for right nodes (targets in edge_index[1])\n",
    "        right_degrees = torch.bincount(\n",
    "            data.edge_index[1],\n",
    "            minlength=num_right\n",
    "        ).float()\n",
    "\n",
    "        # Handle nodes with no connections by ensuring minlength\n",
    "        left_degrees = left_degrees.clamp_min(0)  # No negative degrees\n",
    "        right_degrees = right_degrees.clamp_min(0)\n",
    "\n",
    "        # Compute log(d + 1) for each node\n",
    "        log_degrees_left = torch.log(left_degrees + 1)  # [num_left]\n",
    "        log_degrees_right = torch.log(right_degrees + 1)  # [num_right]\n",
    "\n",
    "        # Accumulate sum and count\n",
    "        log_d_sum_left += torch.sum(log_degrees_left).item()\n",
    "        log_d_sum_right += torch.sum(log_degrees_right).item()\n",
    "        left_count += num_left\n",
    "        right_count += num_right\n",
    "\n",
    "    # Compute average log degree for left and right nodes\n",
    "    avg_log_left = log_d_sum_left / left_count if left_count > 0 else 1.0  # Avoid division by zero\n",
    "    avg_log_right = log_d_sum_right / right_count if right_count > 0 else 1.0\n",
    "\n",
    "    return avg_log_left, avg_log_right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "executionInfo": {
     "elapsed": 5840,
     "status": "error",
     "timestamp": 1747651721238,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "yzQkqu1_u0Ll",
    "outputId": "698f642a-fd6f-4f16-925f-6a1d7613daf5"
   },
   "outputs": [],
   "source": [
    "\n",
    "import multiprocessing as mp\n",
    "import torch.nn.utils as nn_utils\n",
    "print(mp.cpu_count())\n",
    "\n",
    "def pretrain(policy, pretrain_loader):\n",
    "    policy.pre_train_init()\n",
    "    i = 0\n",
    "    while True:\n",
    "        for batch in pretrain_loader:\n",
    "            batch.to(device)\n",
    "            if not policy.pre_train(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features):\n",
    "                break\n",
    "\n",
    "        if policy.pre_train_next() is None:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "def process(policy, data_loader, top_k = [1, 3, 5, 10], optimizer=None):\n",
    "    mean_loss = 0\n",
    "    mean_kacc = np.zeros(len(top_k))\n",
    "    mean_entropy = 0\n",
    "\n",
    "    n_samples_processed = 0\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = policy(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n",
    "            logits = pad_tensor(logits[batch.candidates], batch.nb_candidates)\n",
    "\n",
    "            # if torch.isnan(logits).any():\n",
    "            #     print(\"NaN detected in logits!\")\n",
    "\n",
    "            cross_entropy_loss = F.cross_entropy(logits, batch.candidate_choices, reduction='mean')\n",
    "            entropy = (-F.softmax(logits, dim=-1)*F.log_softmax(logits, dim=-1)).sum(-1).mean()\n",
    "            loss = cross_entropy_loss - entropy_bonus*entropy\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                # nn_utils.clip_grad_norm_(policy.parameters(), 5.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            true_scores = pad_tensor(batch.candidate_scores, batch.nb_candidates)\n",
    "            true_bestscore = true_scores.max(dim=-1, keepdims=True).values\n",
    "\n",
    "            kacc = []\n",
    "            for k in top_k:\n",
    "                if logits.size()[-1] < k:\n",
    "                    kacc.append(1.0)\n",
    "                    continue\n",
    "                pred_top_k = logits.topk(k).indices\n",
    "                pred_top_k_true_scores = true_scores.gather(-1, pred_top_k)\n",
    "                accuracy = (pred_top_k_true_scores == true_bestscore).any(dim=-1).float().mean().item()\n",
    "                kacc.append(accuracy)\n",
    "            kacc = np.asarray(kacc)\n",
    "            mean_loss += cross_entropy_loss.item() * batch.num_graphs\n",
    "            mean_entropy += entropy.item() * batch.num_graphs\n",
    "            mean_kacc += kacc * batch.num_graphs\n",
    "            n_samples_processed += batch.num_graphs\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_kacc /= n_samples_processed\n",
    "    mean_entropy /= n_samples_processed\n",
    "    return mean_loss, mean_kacc, mean_entropy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'problem',\n",
    "        help='MILP instance type to process.',\n",
    "        # choices=['setcover', 'cauctions', 'facilities', 'indset', 'mknapsack'],\n",
    "\n",
    "        choices=['Standard_MTSP', 'MinMax_MTSP', 'Bounded_MTSP', 'JSSP'],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-s', '--seed',\n",
    "        help='Random generator seed.',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-g', '--gpu',\n",
    "        help='CUDA GPU id (-1 for CPU).',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    args = parser.parse_args(['Standard_MTSP'])\n",
    "\n",
    "    ### HYPER PARAMETERS ###\n",
    "    max_epochs = 0\n",
    "    batch_size = 32\n",
    "    pretrain_batch_size = 128\n",
    "    valid_batch_size = 128\n",
    "    lr = 1e-3\n",
    "    entropy_bonus = 0.0\n",
    "    top_k = [1, 3, 5, 10]\n",
    "\n",
    "    problem_folders = {\n",
    "        'setcover': 'setcover/500r_1000c_0.05d',\n",
    "        'cauctions': 'cauctions/100_500',\n",
    "        'facilities': 'facilities/100_100_5',\n",
    "        'indset': 'indset/500_4',\n",
    "        'mknapsack': 'mknapsack/100_6',\n",
    "        'Standard_MTSP': 'Standard_MTSP/12_3',\n",
    "        'JSSP': 'JSSP/6_3',\n",
    "        'MinMax_MTSP': 'MinMax_MTSP/9_3',\n",
    "        'Bounded_MTSP': 'Bounded_MTSP/12_3',\n",
    "    }\n",
    "\n",
    "    problem_folder = problem_folders[args.problem]\n",
    "    running_dir = f\"model/{args.problem}/{args.seed}\"\n",
    "    os.makedirs(running_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    sys.path.insert(0, os.path.abspath(f'model'))\n",
    "\n",
    "    rng = np.random.RandomState(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    ### LOG ###\n",
    "    logfile = os.path.join(running_dir, 'pretrain_log.txt')\n",
    "    if os.path.exists(logfile):\n",
    "        os.remove(logfile)\n",
    "\n",
    "    log(f\"max_epochs: {max_epochs}\", logfile)\n",
    "    log(f\"batch_size: {batch_size}\", logfile)\n",
    "    log(f\"pretrain_batch_size: {pretrain_batch_size}\", logfile)\n",
    "    log(f\"valid_batch_size : {valid_batch_size }\", logfile)\n",
    "    log(f\"lr: {lr}\", logfile)\n",
    "    log(f\"entropy bonus: {entropy_bonus}\", logfile)\n",
    "    log(f\"top_k: {top_k}\", logfile)\n",
    "    log(f\"problem: {args.problem}\", logfile)\n",
    "    log(f\"gpu: {args.gpu}\", logfile)\n",
    "    log(f\"seed {args.seed}\", logfile)\n",
    "\n",
    "    base_path = pathlib.Path('data/samples')\n",
    "\n",
    "    train_files = []\n",
    "\n",
    "    folders = ['1', '2']\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = base_path / problem_folder / 'train' / folder\n",
    "\n",
    "        files = list(folder_path.glob('sample_*.pkl'))\n",
    "\n",
    "        train_files.extend([str(file) for file in files])\n",
    "\n",
    "        print(f\"Loaded {len(files)} files from folder '{folder_path}'.\")\n",
    "\n",
    "        if folder == '1':\n",
    "            print(\"Waiting for 1 seconds before processing the next folder...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    pretrain_files = [f for i, f in enumerate(train_files) if i % 2 == 0]\n",
    "    avg_d_files = [f for i, f in enumerate(train_files) if i % 200 == 0]\n",
    "    valid_files = [str(file) for file in (pathlib.Path(f'data/samples')/problem_folder/'valid').glob('sample_*.pkl')]\n",
    "    pretrain_data = GraphDataset(pretrain_files)\n",
    "    avg_d_data = GraphDataset(avg_d_files)\n",
    "\n",
    "    pretrain_loader = torch_geometric.loader.DataLoader(pretrain_data, pretrain_batch_size, shuffle=False, num_workers=mp.cpu_count())\n",
    "    avg_d_loader = torch_geometric.loader.DataLoader(avg_d_data, batch_size, shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "    valid_data = GraphDataset(valid_files)\n",
    "    valid_loader = torch_geometric.loader.DataLoader(valid_data, valid_batch_size, shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "    ### COMPUTE avg_d ###\n",
    "    avg_log_left, avg_log_right = compute_avg_d_separated(avg_d_loader, device=device)\n",
    "    log(f\"Computed avg_d: Left = {avg_log_left:.4f}, Right = {avg_log_right:.4f}\", logfile)\n",
    "\n",
    "    policy = GNNPolicy(avg_d_left=avg_log_left, avg_d_right=avg_log_right).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=1e-3)\n",
    "    scheduler = Scheduler(optimizer, mode='min', patience=10, factor=0.2, verbose=True)\n",
    "\n",
    "    for epoch in range(max_epochs + 1):\n",
    "        log(f\"EPOCH {epoch}...\", logfile)\n",
    "        if epoch == 0:\n",
    "            n = pretrain(policy, pretrain_loader)\n",
    "            log(f\"PRETRAINED {n} LAYERS\", logfile)\n",
    "        else:\n",
    "            epoch_train_files = rng.choice(train_files, int(np.floor(10000/batch_size))*batch_size, replace=True)\n",
    "            train_data = GraphDataset(epoch_train_files)\n",
    "            train_loader = torch_geometric.data.DataLoader(train_data, batch_size, shuffle=True, num_workers=mp.cpu_count())\n",
    "            train_loss, train_kacc, entropy = process(policy, train_loader, top_k, optimizer)\n",
    "            log(f\"TRAIN LOSS: {train_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, train_kacc)]), logfile)\n",
    "\n",
    "        # TEST\n",
    "        valid_loss, valid_kacc, entropy = process(policy, valid_loader, top_k, None)\n",
    "        log(f\"VALID LOSS: {valid_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, valid_kacc)]), logfile)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        if scheduler.num_bad_epochs == 0:\n",
    "            torch.save(policy.state_dict(), pathlib.Path(running_dir)/'Standard_MTSP_Org_pretrain_PARAM.pkl')\n",
    "            log(f\"  best model so far\", logfile)\n",
    "        elif scheduler.num_bad_epochs == 10:\n",
    "            log(f\"  10 epochs without improvement, decreasing learning rate\", logfile)\n",
    "        elif scheduler.num_bad_epochs == 20:\n",
    "            log(f\"  20 epochs without improvement, early stopping\", logfile)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wja2PVW5qPDa"
   },
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1453978,
     "status": "ok",
     "timestamp": 1747645892424,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "FlawMAVn_bGS",
    "outputId": "9b232d7f-3a8c-4305-b642-43ec02b70918"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def process(policy, data_loader, top_k = [1, 3, 5, 10], optimizer=None):\n",
    "    mean_loss = 0\n",
    "    mean_kacc = np.zeros(len(top_k))\n",
    "    mean_entropy = 0\n",
    "\n",
    "    n_samples_processed = 0\n",
    "    with torch.set_grad_enabled(optimizer is not None):\n",
    "        for batch in data_loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = policy(batch.constraint_features, batch.edge_index, batch.edge_attr, batch.variable_features)\n",
    "            logits = pad_tensor(logits[batch.candidates], batch.nb_candidates)\n",
    "\n",
    "           # if torch.isnan(logits).any():\n",
    "           #     print(\"NaN detected in logits!\")\n",
    "\n",
    "\n",
    "            cross_entropy_loss = F.cross_entropy(logits, batch.candidate_choices, reduction='mean')\n",
    "            entropy = (-F.softmax(logits, dim=-1)*F.log_softmax(logits, dim=-1)).sum(-1).mean()\n",
    "            loss = cross_entropy_loss - entropy_bonus*entropy\n",
    "\n",
    "            if optimizer is not None:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                loss.backward()\n",
    "                # nn_utils.clip_grad_norm_(policy.parameters(), 5.0)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            true_scores = pad_tensor(batch.candidate_scores, batch.nb_candidates)\n",
    "            true_bestscore = true_scores.max(dim=-1, keepdims=True).values\n",
    "\n",
    "            kacc = []\n",
    "            for k in top_k:\n",
    "                if logits.size()[-1] < k:\n",
    "                    kacc.append(1.0)\n",
    "                    continue\n",
    "                pred_top_k = logits.topk(k).indices\n",
    "                pred_top_k_true_scores = true_scores.gather(-1, pred_top_k)\n",
    "                accuracy = (pred_top_k_true_scores == true_bestscore).any(dim=-1).float().mean().item()\n",
    "                kacc.append(accuracy)\n",
    "            kacc = np.asarray(kacc)\n",
    "            mean_loss += cross_entropy_loss.item() * batch.num_graphs\n",
    "            mean_entropy += entropy.item() * batch.num_graphs\n",
    "            mean_kacc += kacc * batch.num_graphs\n",
    "            n_samples_processed += batch.num_graphs\n",
    "\n",
    "    mean_loss /= n_samples_processed\n",
    "    mean_kacc /= n_samples_processed\n",
    "    mean_entropy /= n_samples_processed\n",
    "    return mean_loss, mean_kacc, mean_entropy\n",
    "\n",
    "\n",
    "# from utilities import log, pad_tensor, GraphDataset, Scheduler\n",
    "# from model import GNNPolicy\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'problem',\n",
    "        help='MILP instance type to process.',\n",
    "        # choices=['setcover', 'cauctions', 'facilities', 'indset', 'mknapsack'],\n",
    "\n",
    "        choices=['Standard_MTSP', 'MinMax_MTSP', 'Bounded_MTSP', 'JSSP'],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-s', '--seed',\n",
    "        help='Random generator seed.',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-g', '--gpu',\n",
    "        help='CUDA GPU id (-1 for CPU).',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    args = parser.parse_args(['Standard_MTSP'])  # Adjust arguments as needed\n",
    "\n",
    "    ### HYPER PARAMETERS ###\n",
    "    max_epochs = 1000\n",
    "    batch_size = 32\n",
    "    pretrain_batch_size = 128\n",
    "    valid_batch_size = 128\n",
    "    lr = 1e-3\n",
    "    entropy_bonus = 0.0\n",
    "    top_k = [1, 3, 5, 10]\n",
    "\n",
    "    problem_folders = {\n",
    "        'setcover': 'setcover/500r_1000c_0.05d',\n",
    "        'cauctions': 'cauctions/100_500',\n",
    "        'facilities': 'facilities/100_100_5',\n",
    "        'indset': 'indset/500_4',\n",
    "        'mknapsack': 'mknapsack/100_6',\n",
    "        'MinMax_MTSP': 'MinMax_MTSP/9_3',\n",
    "        'Standard_MTSP': 'Standard_MTSP/12_3',\n",
    "        'JSSP': 'JSSP/6_3',\n",
    "        'Bounded_MTSP': 'Bounded_MTSP/12_3',\n",
    "    }\n",
    "    problem_folder = problem_folders[args.problem]\n",
    "    running_dir = f\"model/{args.problem}/{args.seed}\"\n",
    "    os.makedirs(running_dir, exist_ok=True)\n",
    "\n",
    "    sys.path.insert(0, os.path.abspath(f'model'))\n",
    "\n",
    "    rng = np.random.RandomState(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    ### LOG ###\n",
    "    logfile = os.path.join(running_dir, 'Standard_MTSP_PNA_plot_train_log.txt')\n",
    "    if not os.path.exists(logfile):\n",
    "        with open(logfile, 'w') as f:\n",
    "            f.write(\"\")\n",
    "\n",
    "    log(f\"max_epochs: {max_epochs}\", logfile)\n",
    "    log(f\"batch_size: {batch_size}\", logfile)\n",
    "    log(f\"pretrain_batch_size: {pretrain_batch_size}\", logfile)\n",
    "    log(f\"valid_batch_size : {valid_batch_size}\", logfile)\n",
    "    log(f\"lr: {lr}\", logfile)\n",
    "    log(f\"entropy bonus: {entropy_bonus}\", logfile)\n",
    "    log(f\"top_k: {top_k}\", logfile)\n",
    "    log(f\"problem: {args.problem}\", logfile)\n",
    "    log(f\"gpu: {args.gpu}\", logfile)\n",
    "    log(f\"seed {args.seed}\", logfile)\n",
    "\n",
    "    base_path = pathlib.Path('data/samples')\n",
    "    train_files = []\n",
    "\n",
    "    folders = ['1', '2']\n",
    "\n",
    "    for folder in folders:\n",
    "        folder_path = base_path / problem_folder / 'train' / folder\n",
    "\n",
    "        files = list(folder_path.glob('sample_*.pkl'))\n",
    "\n",
    "        train_files.extend([str(file) for file in files])\n",
    "\n",
    "        print(f\"Loaded {len(files)} files from folder '{folder_path}'.\")\n",
    "\n",
    "        if folder == '1':\n",
    "            print(\"Waiting for 1 seconds before processing the next folder...\")\n",
    "            time.sleep(1)\n",
    "\n",
    "    valid_files = [str(file) for file in (pathlib.Path(f'data/samples')/problem_folder/'valid').glob('sample_*.pkl')]\n",
    " \n",
    "    avg_d_files = [f for i, f in enumerate(train_files) if i % 200 == 0]\n",
    "    avg_d_data = GraphDataset(avg_d_files)\n",
    "    avg_d_loader = torch_geometric.loader.DataLoader(avg_d_data, batch_size, shuffle=False, num_workers=mp.cpu_count())\n",
    "\n",
    "    valid_data = GraphDataset(valid_files)\n",
    "    valid_loader = torch_geometric.loader.DataLoader(valid_data, valid_batch_size, shuffle=False, num_workers=mp.cpu_count()-1)\n",
    "\n",
    "    ## COMPUTE avg_d ###\n",
    "    avg_log_left, avg_log_right = compute_avg_d_separated(avg_d_loader, device=device)\n",
    "    log(f\"Computed avg_d: Left = {avg_log_left:.4f}, Right = {avg_log_right:.4f}\", logfile)\n",
    "\n",
    "\n",
    "    policy = GNNPolicy(avg_d_left=avg_log_left, avg_d_right=avg_log_right).to(device)\n",
    "\n",
    "    # checkpoint_path = pathlib.Path(running_dir) / 'JSSP_Org_pretrain_PARAM.pkl'\n",
    "    # if checkpoint_path.exists():\n",
    "    #     policy.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    #     log(f\"Loaded model parameters from {checkpoint_path}\", logfile)\n",
    "    # else:\n",
    "    #     log(f\"No saved model found at {checkpoint_path}. Exiting.\", logfile)\n",
    "    #     sys.exit(1)\n",
    "\n",
    "    optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n",
    "    scheduler = Scheduler(optimizer, mode='min', patience=5, factor=0.25)\n",
    "\n",
    "    starting_epoch = 1\n",
    "\n",
    "    log(f\"Resuming training from epoch {starting_epoch}...\", logfile)\n",
    "\n",
    "    # Prepare to record metrics\n",
    "    valid_losses = []\n",
    "    valid_accs = {k: [] for k in top_k}\n",
    "\n",
    "    for epoch in range(starting_epoch, max_epochs + 1):\n",
    "        log(f\"EPOCH {epoch}...\", logfile)\n",
    "\n",
    "        for train_step in range(2):\n",
    "            epoch_train_files = rng.choice(train_files, int(np.floor(10000/ batch_size)) * batch_size, replace=True)\n",
    "            epoch_train_data = GraphDataset(epoch_train_files)\n",
    "            epoch_train_loader = torch_geometric.loader.DataLoader(epoch_train_data, batch_size, shuffle=True, num_workers=mp.cpu_count())\n",
    "\n",
    "            train_loss, train_kacc, entropy = process(\n",
    "                policy,\n",
    "                epoch_train_loader,\n",
    "                top_k,\n",
    "                optimizer\n",
    "            )\n",
    "\n",
    "            log(f\"Train Step [{train_step+1}/2] - TRAIN LOSS: {train_loss:0.3f} \" + \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, train_kacc)]), logfile)\n",
    "\n",
    "        # VALIDATION\n",
    "        valid_loss, valid_kacc, entropy = process(\n",
    "            policy,\n",
    "            valid_loader,\n",
    "            top_k,\n",
    "            optimizer=None\n",
    "        )\n",
    "        # record validation metrics\n",
    "        valid_losses.append(valid_loss)\n",
    "        for k_i, k in enumerate(top_k):\n",
    "            valid_accs[k].append(valid_kacc[k_i])\n",
    "\n",
    "        log(f\"VALID LOSS: {valid_loss:0.3f} \" +\n",
    "            \"\".join([f\" acc@{k}: {acc:0.3f}\" for k, acc in zip(top_k, valid_kacc)]),\n",
    "            logfile)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "        if scheduler.num_bad_epochs == 0:\n",
    "            torch.save(policy.state_dict(),\n",
    "                       pathlib.Path(running_dir)/f'{args.problem}_best.pkl')\n",
    "            log(f\"  Best model so far\", logfile)\n",
    "        elif scheduler.num_bad_epochs == 10:\n",
    "            log(f\"  Early stopping after 10 epochs without improvement\", logfile)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml2emKToDgED"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rcKr0msPuzOv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import argparse\n",
    "import csv\n",
    "import time\n",
    "import pickle\n",
    "import ecole\n",
    "import pyscipopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "executionInfo": {
     "elapsed": 78,
     "status": "error",
     "timestamp": 1742700553245,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "x-plLpeSymvu",
    "outputId": "1f39a2df-2062-4a6f-83b3-295bf2f0e3d9"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'problem',\n",
    "        help='MILP instance type to process.',\n",
    "        # choices=['setcover', 'cauctions', 'facilities', 'indset'],\n",
    "\n",
    "        choices=['Standard_MTSP', 'MinMax_MTSP', 'Bounded_MTSP', 'JSSP'],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-g', '--gpu',\n",
    "        help='CUDA GPU id (-1 for CPU).',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    args = parser.parse_args(['Standard_MTSP'])\n",
    "\n",
    "    result_file = f\"{args.problem}_{time.strftime('%Y%m%d-%H%M%S')}.csv\"\n",
    "    instances = []\n",
    "    seeds = [1, 2]\n",
    "\n",
    "    gnn_models = ['pna_gnn']\n",
    "    time_limit = 200\n",
    "\n",
    "    # Define instances based on the selected problem\n",
    "    if args.problem == 'setcover':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/setcover/transfer_500r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/setcover/transfer_1000r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/setcover/transfer_2000r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'cauctions':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/cauctions/transfer_100_500/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/cauctions/transfer_200_1000/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/cauctions/transfer_300_1500/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'facilities':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/facilities/transfer_100_100_5/instance_{i+6}.lp\"} for i in range(10)]\n",
    "        # Removed medium and big instances by setting range to 0\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/facilities/transfer_200_100_5/instance_{i+1}.lp\"} for i in range(5)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/facilities/transfer_400_100_5/instance_{i+1}.lp\"} for i in range(0)]\n",
    "\n",
    "    elif args.problem == 'indset':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/indset/transfer_500_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/indset/transfer_1000_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/indset/transfer_1500_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'MinMax_MTSP':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/MinMax_MTSP/train_9_3/instance_{i+1}.lp\"} for i in range(25)]\n",
    "\n",
    "    elif args.problem == 'JSSP':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/JSSP/train_6_3/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Problem type '{args.problem}' is not implemented.\")\n",
    "\n",
    "    branching_policies = []\n",
    "\n",
    "    for model in gnn_models:\n",
    "        for seed in seeds:\n",
    "            branching_policies.append({\n",
    "                'type': 'gnn',\n",
    "                'name': model,\n",
    "                'seed': seed,\n",
    "            })\n",
    "\n",
    "    print(f\"problem: {args.problem}\")\n",
    "    print(f\"gpu: {args.gpu}\")\n",
    "    print(f\"time limit: {time_limit} s\")\n",
    "\n",
    "    ### PYTORCH SETUP ###\n",
    "    if args.gpu == -1:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "        device = 'cpu'\n",
    "    else:\n",
    "        os.environ['CUDA_VISIBLE_DEVICES'] = f'{args.gpu}'\n",
    "        device = f\"cuda:{0}\"  # Ensure correct CUDA device indexing\n",
    "\n",
    "    # Initialize device\n",
    "    device = torch.device(device if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load and assign models to policies (share models and update parameters)\n",
    "    loaded_models = {}\n",
    "    for policy in branching_policies:\n",
    "        if policy['type'] == 'gnn':\n",
    "            if policy['name'] not in loaded_models:\n",
    "                ### MODEL LOADING ###\n",
    "                if policy['name'] == 'pna_gnn':\n",
    "                    model = GNNPolicy(avg_d_left=1.35, avg_d_right=1.54).to(device) # to be adjusted according to the degree scaler\n",
    "\n",
    "                    model_path = \"/content/drive/MyDrive/Thesis/model/JSSP/0/JSSP_PNA_train_PARAM.pkl\"\n",
    "                # elif policy['name'] == 'org_gnn':\n",
    "                #     model = OrgGNNPolicy().to(device)\n",
    "                #     model_path = \"/content/drive/MyDrive/Thesis/model/JSSP/0/JSSP_Org_train_PARAM.pkl\"\n",
    "                # elif policy['name'] == 'att_gnn':\n",
    "                #     model = AttGNNPolicy().to(device)\n",
    "                #     model_path = \"/content/drive/MyDrive/Thesis/model/JSSP/0/JSSP_Att_pretrain_PARAM.pkl\"\n",
    "\n",
    "                else:\n",
    "                    raise Exception(f\"Unrecognized GNN policy {policy['name']}\")\n",
    "\n",
    "                model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "                loaded_models[policy['name']] = model\n",
    "\n",
    "            policy['model'] = loaded_models[policy['name']]\n",
    "\n",
    "    print(\"Running SCIP...\")\n",
    "\n",
    "    fieldnames = [\n",
    "        'policy',\n",
    "        'seed',\n",
    "        'type',\n",
    "        'instance',\n",
    "        'nnodes',\n",
    "        'nlps',\n",
    "        'stime',\n",
    "        'gap',\n",
    "        'status',\n",
    "        'walltime',\n",
    "        'proctime',\n",
    "    ]\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    scip_parameters = {\n",
    "        'separating/maxrounds': 0,\n",
    "        'presolving/maxrestarts': 0,\n",
    "        'limits/time': time_limit,\n",
    "        'timing/clocktype': 1,\n",
    "        'branching/vanillafullstrong/idempotent': True\n",
    "    }\n",
    "\n",
    "    with open(f\"results/{result_file}\", 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for instance in instances:\n",
    "            print(f\"{instance['type']}: {instance['path']}...\")\n",
    "\n",
    "            for policy in branching_policies:\n",
    "                if policy['type'] == 'gnn':\n",
    "                    # Run the GNN policy\n",
    "                    env = ecole.environment.Branching(\n",
    "                        observation_function=ecole.observation.NodeBipartite(),\n",
    "                        scip_params=scip_parameters\n",
    "                    )\n",
    "                    env.seed(policy['seed'])\n",
    "                    torch.manual_seed(policy['seed'])\n",
    "\n",
    "                    walltime = time.perf_counter()\n",
    "                    proctime = time.process_time()\n",
    "\n",
    "                    observation, action_set, _, done, _ = env.reset(instance['path'])\n",
    "                    while not done:\n",
    "                        with torch.no_grad():\n",
    "                            # Prepare observation tensors\n",
    "                            observation_tensor = (\n",
    "                                torch.from_numpy(observation.row_features.astype(np.float32)).to(device),\n",
    "                                torch.from_numpy(observation.edge_features.indices.astype(np.int64)).to(device),\n",
    "                                torch.from_numpy(observation.edge_features.values.astype(np.float32)).view(-1, 1).to(device),\n",
    "                                torch.from_numpy(observation.variable_features.astype(np.float32)).to(device)\n",
    "                            )\n",
    "\n",
    "                            # Get logits from the model\n",
    "                            logits = policy['model'](*observation_tensor)\n",
    "                            # Select the action with the highest logit\n",
    "                            action = action_set[logits[action_set.astype(np.int64)].argmax()]\n",
    "                            # Step the environment with the selected action\n",
    "                            observation, action_set, _, done, _ = env.step(action)\n",
    "\n",
    "                    walltime = time.perf_counter() - walltime\n",
    "                    proctime = time.process_time() - proctime\n",
    "\n",
    "                    # Retrieve SCIP model metrics\n",
    "                    scip_model = env.model.as_pyscipopt()\n",
    "                    stime = scip_model.getSolvingTime()\n",
    "                    nnodes = scip_model.getNNodes()\n",
    "                    nlps = scip_model.getNLPs()\n",
    "                    gap = scip_model.getGap()\n",
    "                    status = scip_model.getStatus()\n",
    "\n",
    "                    # Write results\n",
    "                    writer.writerow({\n",
    "                        'policy': f\"{policy['type']}:{policy['name']}\",\n",
    "                        'seed': policy['seed'],\n",
    "                        'type': instance['type'],\n",
    "                        'instance': instance['path'],\n",
    "                        'nnodes': nnodes,\n",
    "                        'nlps': nlps,\n",
    "                        'stime': stime,\n",
    "                        'gap': gap,\n",
    "                        'status': status,\n",
    "                        'walltime': walltime,\n",
    "                        'proctime': proctime,\n",
    "                    })\n",
    "                    csvfile.flush()\n",
    "\n",
    "                    print(f\"  {policy['type']}:{policy['name']} {policy['seed']} - {nnodes} nodes {nlps} lps {stime:.2f} ({walltime:.2f} wall {proctime:.2f} proc) s. {status}\")\n",
    "\n",
    "                    # Check if time limit is exceeded\n",
    "                    if stime > time_limit:\n",
    "                        print(f\"  {policy['type']}:{policy['name']} exceeded time limit ({stime:.2f} > {time_limit}). Skipping to next instance.\")\n",
    "                        break  # Skip remaining seeds for this instance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 766
    },
    "executionInfo": {
     "elapsed": 51414,
     "status": "error",
     "timestamp": 1742725572602,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "RdqNBpUOvCSf",
    "outputId": "72569bab-5970-4496-d604-f5eeb943343b"
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'problem',\n",
    "        help='MILP instance type to process.',\n",
    "        # choices=['setcover', 'cauctions', 'facilities', 'indset'],\n",
    "\n",
    "        choices=['Standard_MTSP', 'MinMax_MTSP', 'Bounded_MTSP', 'JSSP'],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-g', '--gpu',\n",
    "        help='CUDA GPU id (-1 for CPU).',\n",
    "        type=int,\n",
    "        default=0,\n",
    "    )\n",
    "    args = parser.parse_args(['JSSP'])\n",
    "\n",
    "    result_file = f\"{args.problem}_{time.strftime('%Y%m%d-%H%M%S')}.csv\"\n",
    "    instances = []\n",
    "    seeds = [1,2]\n",
    "    internal_branchers = ['relpscost']\n",
    "    gnn_models = [''] # Can be supervised\n",
    "    time_limit = 300\n",
    "\n",
    "    if args.problem == 'setcover':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/setcover/transfer_500r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/setcover/transfer_1000r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/setcover/transfer_2000r_1000c_0.05d/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'cauctions':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/cauctions/transfer_100_500/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/cauctions/transfer_200_1000/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/cauctions/transfer_300_1500/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'facilities':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/facilities/transfer_100_100_5/instance_{i+3}.lp\"} for i in range(3)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/facilities/transfer_200_100_5/instance_{i+3}.lp\"} for i in range(0)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/facilities/transfer_400_100_5/instance_{i+3}.lp\"} for i in range(0)]\n",
    "\n",
    "    elif args.problem == 'indset':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/indset/transfer_500_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'medium', 'path': f\"data/instances/indset/transfer_1000_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "        instances += [{'type': 'big', 'path': f\"data/instances/indset/transfer_1500_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'Bounded_MTSP':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/Bounded_MTSP/train_12_3/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    elif args.problem == 'JSSP':\n",
    "        instances += [{'type': 'small', 'path': f\"data/instances/JSSP/train_8_4/instance_{i+1}.lp\"} for i in range(20)]\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    branching_policies = []\n",
    "\n",
    "    # SCIP internal brancher baselines\n",
    "    for brancher in internal_branchers:\n",
    "        for seed in seeds:\n",
    "            branching_policies.append({\n",
    "                    'type': 'internal',\n",
    "                    'name': brancher,\n",
    "                    'seed': seed,\n",
    "             })\n",
    "    # GNN models\n",
    "    for model in gnn_models:\n",
    "        for seed in seeds:\n",
    "            branching_policies.append({\n",
    "                'type': 'gnn',\n",
    "                'name': model,\n",
    "                'seed': seed,\n",
    "            })\n",
    "\n",
    "    print(f\"problem: {args.problem}\")\n",
    "    print(f\"gpu: {args.gpu}\")\n",
    "    print(f\"time limit: {time_limit} s\")\n",
    "\n",
    "    # ### PYTORCH SETUP ###\n",
    "    # if args.gpu == -1:\n",
    "    #     os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "    #     device = 'cpu'\n",
    "    # else:\n",
    "    #     os.environ['CUDA_VISIBLE_DEVICES'] = f'{args.gpu}'\n",
    "    #     device = f\"cuda:0\"\n",
    "\n",
    "    #from model.model import GNNPolicy\n",
    "\n",
    "    # load and assign tensorflow models to policies (share models and update parameters)\n",
    "    loaded_models = {}\n",
    "    loaded_calls = {}\n",
    "    # for policy in branching_policies:\n",
    "    #     if policy['type'] == 'gnn':\n",
    "    #         if policy['name'] not in loaded_models:\n",
    "    #             ### MODEL LOADING ###\n",
    "    #             model = GNNPolicy().to(device)\n",
    "    #             if policy['name'] == 'supervised':\n",
    "    #                 # model.load_state_dict(torch.load(f\"model/{args.problem}/{policy['seed']}/train_params.pkl\"))\n",
    "    #                 model.load_state_dict(torch.load(f\"/content/drive/MyDrive/Thesis/model/facilities/0/facilities_multi_train_params.pkl\"))\n",
    "    #             else:\n",
    "    #                 raise Exception(f\"Unrecognized GNN policy {policy['name']}\")\n",
    "    #             loaded_models[policy['name']] = model\n",
    "\n",
    "    #         policy['model'] = loaded_models[policy['name']]\n",
    "\n",
    "    print(\"running SCIP...\")\n",
    "\n",
    "    fieldnames = [\n",
    "        'policy',\n",
    "        'seed',\n",
    "        'type',\n",
    "        'instance',\n",
    "        'nnodes',\n",
    "        'nlps',\n",
    "        'stime',\n",
    "        'gap',\n",
    "        'status',\n",
    "        'walltime',\n",
    "        'proctime',\n",
    "    ]\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "    scip_parameters = {'separating/maxrounds': 0, 'presolving/maxrestarts': 0, 'limits/time': time_limit,\n",
    "                       'timing/clocktype': 1, 'branching/vanillafullstrong/idempotent': True}\n",
    "\n",
    "    with open(f\"results/{result_file}\", 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for instance in instances:\n",
    "            print(f\"{instance['type']}: {instance['path']}...\")\n",
    "\n",
    "            for policy in branching_policies:\n",
    "                if policy['type'] == 'internal':\n",
    "                    # Run SCIP's default brancher\n",
    "                    env = ecole.environment.Configuring(scip_params={**scip_parameters,\n",
    "                                                        f\"branching/{policy['name']}/priority\": 9999999})\n",
    "                    env.seed(policy['seed'])\n",
    "\n",
    "                    walltime = time.perf_counter()\n",
    "                    proctime = time.process_time()\n",
    "\n",
    "                    env.reset(instance['path'])\n",
    "                    _, _, _, _, _ = env.step({})\n",
    "\n",
    "                    walltime = time.perf_counter() - walltime\n",
    "                    proctime = time.process_time() - proctime\n",
    "\n",
    "                scip_model = env.model.as_pyscipopt()\n",
    "                stime = scip_model.getSolvingTime()\n",
    "                nnodes = scip_model.getNNodes()\n",
    "                nlps = scip_model.getNLPs()\n",
    "                gap = scip_model.getGap()\n",
    "                status = scip_model.getStatus()\n",
    "\n",
    "                writer.writerow({\n",
    "                    'policy': f\"{policy['type']}:{policy['name']}\",\n",
    "                    'seed': policy['seed'],\n",
    "                    'type': instance['type'],\n",
    "                    'instance': instance['path'],\n",
    "                    'nnodes': nnodes,\n",
    "                    'nlps': nlps,\n",
    "                    'stime': stime,\n",
    "                    'gap': gap,\n",
    "                    'status': status,\n",
    "                    'walltime': walltime,\n",
    "                    'proctime': proctime,\n",
    "                })\n",
    "                csvfile.flush()\n",
    "\n",
    "                print(f\"  {policy['type']}:{policy['name']} {policy['seed']} - {nnodes} nodes {nlps} lps {stime:.2f} ({walltime:.2f} wall {proctime:.2f} proc) s. {status}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
