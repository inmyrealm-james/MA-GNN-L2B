{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-uEbGQLz4ZW"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1742097713690,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "2DnspFc_fjTj",
    "outputId": "f099d761-3668-4eb4-bc4e-a74a60dec6e9"
   },
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18239,
     "status": "ok",
     "timestamp": 1742097732144,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "e9zPrCH50Oe_",
    "outputId": "7d9fbcfd-bafb-4e18-e044-9e3082608661"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1742097732160,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "Pcccmof-z4Nd",
    "outputId": "49950027-b63e-430d-f97c-d662aabcf3e3"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjxeUiz90Vi1"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/My Drive/Thesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1742097732178,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "HCTbIdx10d36",
    "outputId": "68b03deb-836c-4f29-d3ed-68ae3b0231da"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzSVehUL0COR"
   },
   "source": [
    "# Generate Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qAHqnrhKzlA"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmRuehPgM_qW"
   },
   "outputs": [],
   "source": [
    "def valid_seed(seed):\n",
    "    \"\"\"Check whether seed is a valid random seed or not.\"\"\"\n",
    "    seed = int(seed)\n",
    "    if seed < 0 or seed > 2**32 - 1:\n",
    "        raise argparse.ArgumentTypeError(\n",
    "                \"seed must be any integer between 0 and 2**32 - 1 inclusive\")\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGFyyA13Ll7-"
   },
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    \"\"\"\n",
    "    Container for a graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    number_of_nodes : int\n",
    "        The number of nodes in the graph.\n",
    "    edges : set of tuples (int, int)\n",
    "        The edges of the graph, where the integers refer to the nodes.\n",
    "    degrees : numpy array of integers\n",
    "        The degrees of the nodes in the graph.\n",
    "    neighbors : dictionary of type {int: set of ints}\n",
    "        The neighbors of each node in the graph.\n",
    "    \"\"\"\n",
    "    def __init__(self, number_of_nodes, edges, degrees, neighbors):\n",
    "        self.number_of_nodes = number_of_nodes\n",
    "        self.edges = edges\n",
    "        self.degrees = degrees\n",
    "        self.neighbors = neighbors\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        The number of nodes in the graph.\n",
    "        \"\"\"\n",
    "        return self.number_of_nodes\n",
    "\n",
    "    def greedy_clique_partition(self):\n",
    "        \"\"\"\n",
    "        Partition the graph into cliques using a greedy algorithm.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of sets\n",
    "            The resulting clique partition.\n",
    "        \"\"\"\n",
    "        cliques = []\n",
    "        leftover_nodes = (-self.degrees).argsort().tolist()\n",
    "\n",
    "        while leftover_nodes:\n",
    "            clique_center, leftover_nodes = leftover_nodes[0], leftover_nodes[1:]\n",
    "            clique = {clique_center}\n",
    "            neighbors = self.neighbors[clique_center].intersection(leftover_nodes)\n",
    "            densest_neighbors = sorted(neighbors, key=lambda x: -self.degrees[x])\n",
    "            for neighbor in densest_neighbors:\n",
    "                # Can you add it to the clique, and maintain cliqueness?\n",
    "                if all([neighbor in self.neighbors[clique_node] for clique_node in clique]):\n",
    "                    clique.add(neighbor)\n",
    "            cliques.append(clique)\n",
    "            leftover_nodes = [node for node in leftover_nodes if node not in clique]\n",
    "\n",
    "        return cliques\n",
    "\n",
    "    @staticmethod\n",
    "    def erdos_renyi(number_of_nodes, edge_probability, random):\n",
    "        \"\"\"\n",
    "        Generate an Erdös-Rényi random graph with a given edge probability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        number_of_nodes : int\n",
    "            The number of nodes in the graph.\n",
    "        edge_probability : float in [0,1]\n",
    "            The probability of generating each edge.\n",
    "        random : numpy.random.RandomState\n",
    "            A random number generator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Graph\n",
    "            The generated graph.\n",
    "        \"\"\"\n",
    "        edges = set()\n",
    "        degrees = np.zeros(number_of_nodes, dtype=int)\n",
    "        neighbors = {node: set() for node in range(number_of_nodes)}\n",
    "        for edge in combinations(np.arange(number_of_nodes), 2):\n",
    "            if random.uniform() < edge_probability:\n",
    "                edges.add(edge)\n",
    "                degrees[edge[0]] += 1\n",
    "                degrees[edge[1]] += 1\n",
    "                neighbors[edge[0]].add(edge[1])\n",
    "                neighbors[edge[1]].add(edge[0])\n",
    "        graph = Graph(number_of_nodes, edges, degrees, neighbors)\n",
    "        return graph\n",
    "\n",
    "    @staticmethod\n",
    "    def barabasi_albert(number_of_nodes, affinity, random):\n",
    "        \"\"\"\n",
    "        Generate a Barabási-Albert random graph with a given edge probability.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        number_of_nodes : int\n",
    "            The number of nodes in the graph.\n",
    "        affinity : integer >= 1\n",
    "            The number of nodes each new node will be attached to, in the sampling scheme.\n",
    "        random : numpy.random.RandomState\n",
    "            A random number generator.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Graph\n",
    "            The generated graph.\n",
    "        \"\"\"\n",
    "        assert affinity >= 1 and affinity < number_of_nodes\n",
    "\n",
    "        edges = set()\n",
    "        degrees = np.zeros(number_of_nodes, dtype=int)\n",
    "        neighbors = {node: set() for node in range(number_of_nodes)}\n",
    "        for new_node in range(affinity, number_of_nodes):\n",
    "            # first node is connected to all previous ones (star-shape)\n",
    "            if new_node == affinity:\n",
    "                neighborhood = np.arange(new_node)\n",
    "            # remaining nodes are picked stochastically\n",
    "            else:\n",
    "                neighbor_prob = degrees[:new_node] / (2*len(edges))\n",
    "                neighborhood = random.choice(new_node, affinity, replace=False, p=neighbor_prob)\n",
    "            for node in neighborhood:\n",
    "                edges.add((node, new_node))\n",
    "                degrees[node] += 1\n",
    "                degrees[new_node] += 1\n",
    "                neighbors[node].add(new_node)\n",
    "                neighbors[new_node].add(node)\n",
    "\n",
    "        graph = Graph(number_of_nodes, edges, degrees, neighbors)\n",
    "        return graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UOz114sWL5WN"
   },
   "source": [
    "generate_indset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7Yy98TfLlsP"
   },
   "outputs": [],
   "source": [
    "def generate_indset(graph, filename):\n",
    "    \"\"\"\n",
    "    Generate a Maximum Independent Set (also known as Maximum Stable Set) instance\n",
    "    in CPLEX LP format from a previously generated graph.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    graph : Graph\n",
    "        The graph from which to build the independent set problem.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    \"\"\"\n",
    "    cliques = graph.greedy_clique_partition()\n",
    "    inequalities = set(graph.edges)\n",
    "    for clique in cliques:\n",
    "        clique = tuple(sorted(clique))\n",
    "        for edge in combinations(clique, 2):\n",
    "            inequalities.remove(edge)\n",
    "        if len(clique) > 1:\n",
    "            inequalities.add(clique)\n",
    "\n",
    "    # Put trivial inequalities for nodes that didn't appear\n",
    "    # in the constraints, otherwise SCIP will complain\n",
    "    used_nodes = set()\n",
    "    for group in inequalities:\n",
    "        used_nodes.update(group)\n",
    "    for node in range(10):\n",
    "        if node not in used_nodes:\n",
    "            inequalities.add((node,))\n",
    "\n",
    "    with open(filename, 'w') as lp_file:\n",
    "        lp_file.write(\"maximize\\nOBJ:\" + \"\".join([f\" + 1 x{node+1}\" for node in range(len(graph))]) + \"\\n\")\n",
    "        lp_file.write(\"\\nsubject to\\n\")\n",
    "        for count, group in enumerate(inequalities):\n",
    "            lp_file.write(f\"C{count+1}:\" + \"\".join([f\" + x{node+1}\" for node in sorted(group)]) + \" <= 1\\n\")\n",
    "        lp_file.write(\"\\nbinary\\n\" + \" \".join([f\"x{node+1}\" for node in range(len(graph))]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thYT6V_0L0tH"
   },
   "source": [
    "generate_setcover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6RP6J4r0L0C2"
   },
   "outputs": [],
   "source": [
    "def generate_setcover(nrows, ncols, density, filename, rng, max_coef=100):\n",
    "    \"\"\"\n",
    "    Generates a setcover instance with specified characteristics, and writes\n",
    "    it to a file in the LP format.\n",
    "\n",
    "    Approach described in:\n",
    "    E.Balas and A.Ho, Set covering algorithms using cutting planes, heuristics,\n",
    "    and subgradient optimization: A computational study, Mathematical\n",
    "    Programming, 12 (1980), 37-60.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nrows : int\n",
    "        Desired number of rows\n",
    "    ncols : int\n",
    "        Desired number of columns\n",
    "    density: float between 0 (excluded) and 1 (included)\n",
    "        Desired density of the constraint matrix\n",
    "    filename: str\n",
    "        File to which the LP will be written\n",
    "    rng: numpy.random.RandomState\n",
    "        Random number generator\n",
    "    max_coef: int\n",
    "        Maximum objective coefficient (>=1)\n",
    "    \"\"\"\n",
    "    nnzrs = int(nrows * ncols * density)\n",
    "\n",
    "    assert nnzrs >= nrows  # at least 1 col per row\n",
    "    assert nnzrs >= 2 * ncols  # at leats 2 rows per col\n",
    "\n",
    "    # compute number of rows per column\n",
    "    indices = rng.choice(ncols, size=nnzrs)  # random column indexes\n",
    "    indices[:2 * ncols] = np.repeat(np.arange(ncols), 2)  # force at leats 2 rows per col\n",
    "    _, col_nrows = np.unique(indices, return_counts=True)\n",
    "\n",
    "    # for each column, sample random rows\n",
    "    indices[:nrows] = rng.permutation(nrows) # force at least 1 column per row\n",
    "    i = 0\n",
    "    indptr = [0]\n",
    "    for n in col_nrows:\n",
    "\n",
    "        # empty column, fill with random rows\n",
    "        if i >= nrows:\n",
    "            indices[i:i+n] = rng.choice(nrows, size=n, replace=False)\n",
    "\n",
    "        # partially filled column, complete with random rows among remaining ones\n",
    "        elif i + n > nrows:\n",
    "            remaining_rows = np.setdiff1d(np.arange(nrows), indices[i:nrows], assume_unique=True)\n",
    "            indices[nrows:i+n] = rng.choice(remaining_rows, size=i+n-nrows, replace=False)\n",
    "\n",
    "        i += n\n",
    "        indptr.append(i)\n",
    "\n",
    "    # objective coefficients\n",
    "    c = rng.randint(max_coef, size=ncols) + 1\n",
    "\n",
    "    # sparce CSC to sparse CSR matrix\n",
    "    A = scipy.sparse.csc_matrix(\n",
    "            (np.ones(len(indices), dtype=int), indices, indptr),\n",
    "            shape=(nrows, ncols)).tocsr()\n",
    "    indices = A.indices\n",
    "    indptr = A.indptr\n",
    "\n",
    "    # write problem\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"minimize\\nOBJ:\")\n",
    "        file.write(\"\".join([f\" +{c[j]} x{j+1}\" for j in range(ncols)]))\n",
    "\n",
    "        file.write(\"\\n\\nsubject to\\n\")\n",
    "        for i in range(nrows):\n",
    "            row_cols_str = \"\".join([f\" +1 x{j+1}\" for j in indices[indptr[i]:indptr[i+1]]])\n",
    "            file.write(f\"C{i}:\" + row_cols_str + f\" >= 1\\n\")\n",
    "\n",
    "        file.write(\"\\nbinary\\n\")\n",
    "        file.write(\"\".join([f\" x{j+1}\" for j in range(ncols)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMAj3hKeL-tL"
   },
   "outputs": [],
   "source": [
    "def generate_cauctions(random, filename, n_items=100, n_bids=500, min_value=1, max_value=100,\n",
    "                       value_deviation=0.5, add_item_prob=0.7, max_n_sub_bids=5,\n",
    "                       additivity=0.2, budget_factor=1.5, resale_factor=0.5,\n",
    "                       integers=False, warnings=False):\n",
    "    \"\"\"\n",
    "    Generate a Combinatorial Auction problem following the 'arbitrary' scheme found in section 4.3. of\n",
    "        Kevin Leyton-Brown, Mark Pearson, and Yoav Shoham. (2000).\n",
    "        Towards a universal test suite for combinatorial auction algorithms.\n",
    "        Proceedings of ACM Conference on Electronic Commerce (EC-00) 66-76.\n",
    "\n",
    "    Saves it as a CPLEX LP file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random : numpy.random.RandomState\n",
    "        A random number generator.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    n_items : int\n",
    "        The number of items.\n",
    "    n_bids : int\n",
    "        The number of bids.\n",
    "    min_value : int\n",
    "        The minimum resale value for an item.\n",
    "    max_value : int\n",
    "        The maximum resale value for an item.\n",
    "    value_deviation : int\n",
    "        The deviation allowed for each bidder's private value of an item, relative from max_value.\n",
    "    add_item_prob : float in [0, 1]\n",
    "        The probability of adding a new item to an existing bundle.\n",
    "    max_n_sub_bids : int\n",
    "        The maximum number of substitutable bids per bidder (+1 gives the maximum number of bids per bidder).\n",
    "    additivity : float\n",
    "        Additivity parameter for bundle prices. Note that additivity < 0 gives sub-additive bids, while additivity > 0 gives super-additive bids.\n",
    "    budget_factor : float\n",
    "        The budget factor for each bidder, relative to their initial bid's price.\n",
    "    resale_factor : float\n",
    "        The resale factor for each bidder, relative to their initial bid's resale value.\n",
    "    integers : logical\n",
    "        Should bid's prices be integral ?\n",
    "    warnings : logical\n",
    "        Should warnings be printed ?\n",
    "    \"\"\"\n",
    "\n",
    "    assert min_value >= 0 and max_value >= min_value\n",
    "    assert add_item_prob >= 0 and add_item_prob <= 1\n",
    "\n",
    "    def choose_next_item(bundle_mask, interests, compats, add_item_prob, random):\n",
    "        n_items = len(interests)\n",
    "        prob = (1 - bundle_mask) * interests * compats[bundle_mask, :].mean(axis=0)\n",
    "        prob /= prob.sum()\n",
    "        return random.choice(n_items, p=prob)\n",
    "\n",
    "    # common item values (resale price)\n",
    "    values = min_value + (max_value - min_value) * random.rand(n_items)\n",
    "\n",
    "    # item compatibilities\n",
    "    compats = np.triu(random.rand(n_items, n_items), k=1)\n",
    "    compats = compats + compats.transpose()\n",
    "    compats = compats / compats.sum(1)\n",
    "\n",
    "    bids = []\n",
    "    n_dummy_items = 0\n",
    "\n",
    "    # create bids, one bidder at a time\n",
    "    while len(bids) < n_bids:\n",
    "\n",
    "        # bidder item values (buy price) and interests\n",
    "        private_interests = random.rand(n_items)\n",
    "        private_values = values + max_value * value_deviation * (2 * private_interests - 1)\n",
    "\n",
    "        # substitutable bids of this bidder\n",
    "        bidder_bids = {}\n",
    "\n",
    "        # generate initial bundle, choose first item according to bidder interests\n",
    "        prob = private_interests / private_interests.sum()\n",
    "        item = random.choice(n_items, p=prob)\n",
    "        bundle_mask = np.full(n_items, 0)\n",
    "        bundle_mask[item] = 1\n",
    "\n",
    "        # add additional items, according to bidder interests and item compatibilities\n",
    "        while random.rand() < add_item_prob:\n",
    "            # stop when bundle full (no item left)\n",
    "            if bundle_mask.sum() == n_items:\n",
    "                break\n",
    "            item = choose_next_item(bundle_mask, private_interests, compats, add_item_prob, random)\n",
    "            bundle_mask[item] = 1\n",
    "\n",
    "        bundle = np.nonzero(bundle_mask)[0]\n",
    "\n",
    "        # compute bundle price with value additivity\n",
    "        price = private_values[bundle].sum() + np.power(len(bundle), 1 + additivity)\n",
    "        if integers:\n",
    "            price = int(price)\n",
    "\n",
    "        # drop negativaly priced bundles\n",
    "        if price < 0:\n",
    "            if warnings:\n",
    "                print(\"warning: negatively priced bundle avoided\")\n",
    "            continue\n",
    "\n",
    "        # bid on initial bundle\n",
    "        bidder_bids[frozenset(bundle)] = price\n",
    "\n",
    "        # generate candidates substitutable bundles\n",
    "        sub_candidates = []\n",
    "        for item in bundle:\n",
    "\n",
    "            # at least one item must be shared with initial bundle\n",
    "            bundle_mask = np.full(n_items, 0)\n",
    "            bundle_mask[item] = 1\n",
    "\n",
    "            # add additional items, according to bidder interests and item compatibilities\n",
    "            while bundle_mask.sum() < len(bundle):\n",
    "                item = choose_next_item(bundle_mask, private_interests, compats, add_item_prob, random)\n",
    "                bundle_mask[item] = 1\n",
    "\n",
    "            sub_bundle = np.nonzero(bundle_mask)[0]\n",
    "\n",
    "            # compute bundle price with value additivity\n",
    "            sub_price = private_values[sub_bundle].sum() + np.power(len(sub_bundle), 1 + additivity)\n",
    "            if integers:\n",
    "                sub_price = int(sub_price)\n",
    "\n",
    "            sub_candidates.append((sub_bundle, sub_price))\n",
    "\n",
    "        # filter valid candidates, higher priced candidates first\n",
    "        budget = budget_factor * price\n",
    "        min_resale_value = resale_factor * values[bundle].sum()\n",
    "        for bundle, price in [\n",
    "            sub_candidates[i] for i in np.argsort([-price for bundle, price in sub_candidates])]:\n",
    "\n",
    "            if len(bidder_bids) >= max_n_sub_bids + 1 or len(bids) + len(bidder_bids) >= n_bids:\n",
    "                break\n",
    "\n",
    "            if price < 0:\n",
    "                if warnings:\n",
    "                    print(\"warning: negatively priced substitutable bundle avoided\")\n",
    "                continue\n",
    "\n",
    "            if price > budget:\n",
    "                if warnings:\n",
    "                    print(\"warning: over priced substitutable bundle avoided\")\n",
    "                continue\n",
    "\n",
    "            if values[bundle].sum() < min_resale_value:\n",
    "                if warnings:\n",
    "                    print(\"warning: substitutable bundle below min resale value avoided\")\n",
    "                continue\n",
    "\n",
    "            if frozenset(bundle) in bidder_bids:\n",
    "                if warnings:\n",
    "                    print(\"warning: duplicated substitutable bundle avoided\")\n",
    "                continue\n",
    "\n",
    "            bidder_bids[frozenset(bundle)] = price\n",
    "\n",
    "        # add XOR constraint if needed (dummy item)\n",
    "        if len(bidder_bids) > 2:\n",
    "            dummy_item = [n_items + n_dummy_items]\n",
    "            n_dummy_items += 1\n",
    "        else:\n",
    "            dummy_item = []\n",
    "\n",
    "        # place bids\n",
    "        for bundle, price in bidder_bids.items():\n",
    "            bids.append((list(bundle) + dummy_item, price))\n",
    "\n",
    "    # generate the LP file\n",
    "    with open(filename, 'w') as file:\n",
    "        bids_per_item = [[] for item in range(n_items + n_dummy_items)]\n",
    "\n",
    "        file.write(\"maximize\\nOBJ:\")\n",
    "        for i, bid in enumerate(bids):\n",
    "            bundle, price = bid\n",
    "            file.write(f\" +{price} x{i+1}\")\n",
    "            for item in bundle:\n",
    "                bids_per_item[item].append(i)\n",
    "\n",
    "        file.write(\"\\n\\nsubject to\\n\")\n",
    "        for item_bids in bids_per_item:\n",
    "            if item_bids:\n",
    "                for i in item_bids:\n",
    "                    file.write(f\" +1 x{i+1}\")\n",
    "                file.write(f\" <= 1\\n\")\n",
    "\n",
    "        file.write(\"\\nbinary\\n\")\n",
    "        for i in range(len(bids)):\n",
    "            file.write(f\" x{i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XAkMR24fMExY"
   },
   "outputs": [],
   "source": [
    "def generate_capacited_facility_location(random, filename, n_customers, n_facilities, ratio):\n",
    "    \"\"\"\n",
    "    Generate a Capacited Facility Location problem following\n",
    "        Cornuejols G, Sridharan R, Thizy J-M (1991)\n",
    "        A Comparison of Heuristics and Relaxations for the Capacitated Plant Location Problem.\n",
    "        European Journal of Operations Research 50:280-297.\n",
    "\n",
    "    Saves it as a CPLEX LP file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random : numpy.random.RandomState\n",
    "        A random number generator.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    n_customers: int\n",
    "        The desired number of customers.\n",
    "    n_facilities: int\n",
    "        The desired number of facilities.\n",
    "    ratio: float\n",
    "        The desired capacity / demand ratio.\n",
    "    \"\"\"\n",
    "    c_x = rng.rand(n_customers)\n",
    "    c_y = rng.rand(n_customers)\n",
    "\n",
    "    f_x = rng.rand(n_facilities)\n",
    "    f_y = rng.rand(n_facilities)\n",
    "\n",
    "    demands = rng.randint(5, 35+1, size=n_customers)\n",
    "    capacities = rng.randint(10, 160+1, size=n_facilities)\n",
    "    fixed_costs = rng.randint(100, 110+1, size=n_facilities) * np.sqrt(capacities) \\\n",
    "            + rng.randint(90+1, size=n_facilities)\n",
    "    fixed_costs = fixed_costs.astype(int)\n",
    "\n",
    "    total_demand = demands.sum()\n",
    "    total_capacity = capacities.sum()\n",
    "\n",
    "    # adjust capacities according to ratio\n",
    "    capacities = capacities * ratio * total_demand / total_capacity\n",
    "    capacities = capacities.astype(int)\n",
    "    total_capacity = capacities.sum()\n",
    "\n",
    "    # transportation costs\n",
    "    trans_costs = np.sqrt(\n",
    "            (c_x.reshape((-1, 1)) - f_x.reshape((1, -1))) ** 2 \\\n",
    "            + (c_y.reshape((-1, 1)) - f_y.reshape((1, -1))) ** 2) * 10 * demands.reshape((-1, 1))\n",
    "\n",
    "    # write problem\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"minimize\\nobj:\")\n",
    "        file.write(\"\".join([f\" +{trans_costs[i, j]} x_{i+1}_{j+1}\" for i in range(n_customers) for j in range(n_facilities)]))\n",
    "        file.write(\"\".join([f\" +{fixed_costs[j]} y_{j+1}\" for j in range(n_facilities)]))\n",
    "\n",
    "        file.write(\"\\n\\nsubject to\\n\")\n",
    "        for i in range(n_customers):\n",
    "            file.write(f\"demand_{i+1}:\" + \"\".join([f\" -1 x_{i+1}_{j+1}\" for j in range(n_facilities)]) + f\" <= -1\\n\")\n",
    "        for j in range(n_facilities):\n",
    "            file.write(f\"capacity_{j+1}:\" + \"\".join([f\" +{demands[i]} x_{i+1}_{j+1}\" for i in range(n_customers)]) + f\" -{capacities[j]} y_{j+1} <= 0\\n\")\n",
    "\n",
    "        # optional constraints for LP relaxation tightening\n",
    "        file.write(\"total_capacity:\" + \"\".join([f\" -{capacities[j]} y_{j+1}\" for j in range(n_facilities)]) + f\" <= -{total_demand}\\n\")\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_facilities):\n",
    "                file.write(f\"affectation_{i+1}_{j+1}: +1 x_{i+1}_{j+1} -1 y_{j+1} <= 0\")\n",
    "\n",
    "        file.write(\"\\nbounds\\n\")\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_facilities):\n",
    "                file.write(f\"0 <= x_{i+1}_{j+1} <= 1\\n\")\n",
    "\n",
    "        file.write(\"\\nbinary\\n\")\n",
    "        file.write(\"\".join([f\" y_{j+1}\" for j in range(n_facilities)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9yyddhV3PcH7"
   },
   "source": [
    "# MTSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Swdm9PoxPbgx"
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_Standard_MTSP(random, filename, n_customers,m_salesman):\n",
    "    \"\"\"\n",
    "    Generate a MTSP problem following\n",
    "\n",
    "        Bektas, T.: The multiple traveling salesman problem: an overview of\n",
    "    formulations and solution procedures. Omega, 34(3) (2006), pp. 209–219..\n",
    "\n",
    "    Saves it as a CPLEX LP file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random : numpy.random.RandomState\n",
    "        A random number generator.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    n_customers: int\n",
    "        The desired number of customers.\n",
    "    \"\"\"\n",
    "\n",
    "    c_x = rng.rand(n_customers)\n",
    "    c_y = rng.rand(n_customers)\n",
    "\n",
    "    f_x = c_x\n",
    "    f_y = c_y\n",
    "\n",
    "\n",
    "    # transportation costs\n",
    "    trans_costs = np.sqrt(\n",
    "            (c_x.reshape((-1, 1)) - f_x.reshape((1, -1))) ** 2 \\\n",
    "            + (c_y.reshape((-1, 1)) - f_y.reshape((1, -1))) ** 2)\n",
    "\n",
    "    # write problem\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"Minimize\\n obj:\")\n",
    "\n",
    "        file.write(\"\".join([f\" + {trans_costs[i, j]} x_{i+1}_{j+1}\" for i in range(n_customers) for j in range(n_customers)]))\n",
    "\n",
    "        file.write(\"\\nSubject To\\n\")\n",
    "\n",
    "        cnt = 0\n",
    "        cnt = cnt+1\n",
    "\n",
    "        file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{1}_{j+1}\" for j in range(1,n_customers)]) + f\" = {m_salesman}\\n\")\n",
    "\n",
    "        cnt = cnt+1\n",
    "        file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{j+1}_{1}\" for j in range(1,n_customers)]) + f\" = {m_salesman}\\n\")\n",
    "\n",
    "        cnt = cnt+1\n",
    "        file.write(f\" c{cnt}: u_{1} = 0\\n\")\n",
    "\n",
    "        nvisit = n_customers/m_salesman +1\n",
    "\n",
    "        for i in range(1,n_customers):\n",
    "            cnt = cnt+1\n",
    "            file.write(f\" c{cnt}: u_{i+1} >= 1\\n\")\n",
    "            cnt = cnt+1\n",
    "            file.write(f\" c{cnt}: u_{i+1} <= {nvisit-1}\\n\")\n",
    "\n",
    "\n",
    "        for i in range(0,n_customers):\n",
    "            for j in range(0,n_customers):\n",
    "                cnt = cnt+1\n",
    "                if i == j:\n",
    "                    file.write(f\" c{cnt}: x_{i+1}_{j+1} = 0\\n\")\n",
    "                else:\n",
    "                    file.write(f\" c{cnt}: x_{i+1}_{j+1} >= 0\\n\")\n",
    "                    cnt = cnt+1\n",
    "                    file.write(f\" c{cnt}: x_{i+1}_{j+1} <= 1\\n\")\n",
    "\n",
    "\n",
    "        for i in range(1,n_customers):\n",
    "            cnt = cnt+1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{i+1}_{j+1}\" for j in range(n_customers)]) + f\" = 1\\n\")\n",
    "\n",
    "        for j in range(1,n_customers):\n",
    "            cnt = cnt+1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{i+1}_{j+1}\" for i in range(n_customers)]) + f\" = 1\\n\")\n",
    "\n",
    "\n",
    "        for i in range(1,n_customers):\n",
    "            cnt = cnt+1\n",
    "            file.write(f\" c{cnt}: x_{1}_{i+1} + x_{i+1}_{1} <= 1\\n\")\n",
    "\n",
    "        for i in range(1,n_customers):\n",
    "            for j in range(1,n_customers):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                    file.write(f\" c{cnt}: {n_customers-m_salesman} x_{i+1}_{j+1} <= {n_customers-m_salesman-1} \\n\")\n",
    "                else:\n",
    "                    cnt = cnt+1\n",
    "                    file.write(f\" c{cnt}: {n_customers-m_salesman} x_{i+1}_{j+1} + u_{i+1} - u_{j+1} <= {n_customers-m_salesman-1} \\n\")\n",
    "\n",
    "        file.write(\"\\nBounds\\n\")\n",
    "        #for i in range(n_customers):\n",
    "        #    file.write(f\" 1 <= u_{i+1} <= {n_customers}\\n\")\n",
    "\n",
    "        #for i in range(n_customers):\n",
    "        #    for j in range(n_customers):\n",
    "        #        file.write(f\" 0 <= x_{i+1}_{j+1} <= 1\\n\")\n",
    "\n",
    "        file.write(\"\\nGenerals\\n\")\n",
    "\n",
    "        #file.write(\"\\nbinary\\n\")\n",
    "\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_customers):\n",
    "                file.write(f\" x_{i+1}_{j+1} \")\n",
    "        for i in range(n_customers):\n",
    "            file.write(f\" u_{i+1} \")\n",
    "\n",
    "        #file.write(\"\".join([f\" y_{j+1}\" for j in range(n_facilities)]))\n",
    "        file.write(\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V4F8KmQxeyCG"
   },
   "outputs": [],
   "source": [
    "def generate_MinMax_MTSP(random, filename, n_customers, m_salesman):\n",
    "    \"\"\"\n",
    "    Generate a MinMax MTSP problem following\n",
    "\n",
    "    Necula, R., Breaban, M., Raschip, M.: Tackling the Bi-criteria Facet of Multiple Traveling Salesman Problem with Ant Colony Systems, 27th International Conference on Tools with Artificial Intelligence (ICTAI), 9-11 November, Vietri sul Mare, Italy, pp. 873-880, 2015\n",
    "\n",
    "    Saves it as a CPLEX LP file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    random : numpy.random.RandomState\n",
    "        A random number generator.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    n_customers: int\n",
    "        The desired number of customers (including the depot as node 1).\n",
    "    m_salesman: int\n",
    "        The number of salesmen.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Generate random customer coordinates\n",
    "    c_x = random.rand(n_customers)\n",
    "    c_y = random.rand(n_customers)\n",
    "\n",
    "    f_x = c_x\n",
    "    f_y = c_y\n",
    "\n",
    "    # Transportation costs (Euclidean distance)\n",
    "    trans_costs = np.sqrt(\n",
    "        (c_x.reshape((-1, 1)) - f_x.reshape((1, -1))) ** 2 +\n",
    "        (c_y.reshape((-1, 1)) - f_y.reshape((1, -1))) ** 2\n",
    "    )\n",
    "\n",
    "    # Calculate maximum number of customers per salesman (excluding depot)\n",
    "    nvisit = int(np.ceil((n_customers - 1) / m_salesman)) + 1\n",
    "\n",
    "    # Write problem to file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"Minimize\\n obj:\")\n",
    "        file.write(f\"1 W_{1}\\n\")\n",
    "        file.write(\"\\nSubject To\\n\")\n",
    "\n",
    "        cnt = 0\n",
    "\n",
    "        # Each salesman must leave the depot (node 1)\n",
    "        for k in range(m_salesman):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{1}_{j+1}_{k+1}\" for j in range(1, n_customers)]) + \" = 1\\n\")\n",
    "\n",
    "        # Each salesman must return to the depot (node 1)\n",
    "        for k in range(m_salesman):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{j+1}_{1}_{k+1}\" for j in range(1, n_customers)]) + \" = 1\\n\")\n",
    "\n",
    "        # Set u_1 (depot) to 0\n",
    "        cnt += 1\n",
    "        file.write(f\" c{cnt}: u_{1} = 0\\n\")\n",
    "\n",
    "        # Set bounds on u variables (position in route)\n",
    "        for i in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}: u_{i+1} >= 1\\n\")\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}: u_{i+1} <= {nvisit-1}\\n\")\n",
    "\n",
    "        # No self-loops: force x_{i,i,k} = 0\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_customers):\n",
    "                if i == j:\n",
    "                    for k in range(m_salesman):\n",
    "                        cnt += 1\n",
    "                        file.write(f\" c{cnt}: x_{i+1}_{j+1}_{k+1} = 0\\n\")\n",
    "\n",
    "        # Each customer (non-depot) must be visited exactly once\n",
    "        for j in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{i+1}_{j+1}_{k+1}\" for i in range(n_customers) for k in range(m_salesman)]) + \" = 1\\n\")\n",
    "\n",
    "        # Each customer (non-depot) must be left exactly once\n",
    "        for i in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{i+1}_{j+1}_{k+1}\" for j in range(n_customers) for k in range(m_salesman)]) + \" = 1\\n\")\n",
    "\n",
    "        # Flow conservation for each customer and salesman\n",
    "        for j in range(1, n_customers):\n",
    "            for k in range(m_salesman):\n",
    "                cnt += 1\n",
    "                file.write(f\" c{cnt}:\" + \"\".join([f\" + 1 x_{i+1}_{j+1}_{k+1} - 1 x_{j+1}_{i+1}_{k+1}\" for i in range(n_customers)]) + \" = 0\\n\")\n",
    "\n",
    "        # Subtour elimination constraints (MTZ formulation)\n",
    "        for i in range(1, n_customers):\n",
    "            for j in range(1, n_customers):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                cnt += 1\n",
    "                file.write(\n",
    "                    f\" c{cnt}: + u_{i+1} - u_{j+1} \" +\n",
    "                    \"\".join([f\" + {nvisit} x_{i+1}_{j+1}_{k+1}\" for k in range(m_salesman)]) +\n",
    "                    f\" <= {nvisit-1}\\n\"\n",
    "                )\n",
    "\n",
    "        # MinMax objective constraints: ensure each salesman’s tour cost does not exceed W_1\n",
    "        for k in range(m_salesman):\n",
    "            cnt += 1\n",
    "            file.write(\n",
    "                f\" c{cnt}:\" +\n",
    "                \"\".join([f\" + {trans_costs[i, j]} x_{i+1}_{j+1}_{k+1}\"\n",
    "                         for i in range(n_customers) for j in range(n_customers) if i != j]) +\n",
    "                \" - 1 W_1 <= 0\\n\"\n",
    "            )\n",
    "\n",
    "        file.write(\"\\nBounds\\n\")\n",
    "        file.write(\"0 <= W_1 \\n\")\n",
    "\n",
    "        file.write(\"\\nGenerals\\n\")\n",
    "        # u variables are general integers\n",
    "        for i in range(n_customers):\n",
    "            file.write(f\" u_{i+1} \")\n",
    "\n",
    "        file.write(\"\\nBinary\\n\")\n",
    "        # x variables are binary (only declare those not fixed to 0)\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_customers):\n",
    "                if i != j:\n",
    "                    for k in range(m_salesman):\n",
    "                        file.write(f\" x_{i+1}_{j+1}_{k+1} \")\n",
    "\n",
    "        file.write(\"\\nEnd\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "md_PAcpzPy2W"
   },
   "outputs": [],
   "source": [
    "def generate_Bounded_MTSP(rng, filename, n_customers, m_salesman, L, K):\n",
    "    \"\"\"\n",
    "    Generate a bounded MTSP problem and save as a CPLEX LP file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rng : numpy.random.RandomState\n",
    "        A random number generator.\n",
    "    filename : str\n",
    "        Path to the file to save.\n",
    "    n_customers: int\n",
    "        The desired number of customers.\n",
    "    m_salesman : int\n",
    "        Number of salesmen.\n",
    "    L : int or float\n",
    "        Parameter used in capacity/sequencing constraints.\n",
    "    K : int or float\n",
    "        Parameter used in capacity/sequencing constraints.\n",
    "    \"\"\"\n",
    "    import numpy as np  # ensure numpy is imported\n",
    "\n",
    "    # Use the passed random generator\n",
    "    c_x = rng.rand(n_customers)\n",
    "    c_y = rng.rand(n_customers)\n",
    "\n",
    "    # Here, f_x and f_y are set equal to c_x and c_y.\n",
    "    f_x = c_x\n",
    "    f_y = c_y\n",
    "\n",
    "    # Transportation cost: Euclidean distance between customers\n",
    "    trans_costs = np.sqrt(\n",
    "        (c_x.reshape((-1, 1)) - f_x.reshape((1, -1))) ** 2 +\n",
    "        (c_y.reshape((-1, 1)) - f_y.reshape((1, -1))) ** 2\n",
    "    )\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        # Objective: minimize total transportation cost\n",
    "        file.write(\"Minimize\\n obj:\")\n",
    "        obj_terms = []\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_customers):\n",
    "                if i != j:  # Exclude self-loops\n",
    "                    obj_terms.append(f\" + {trans_costs[i, j]} x_{i+1}_{j+1}\")\n",
    "        file.write(\"\".join(obj_terms))\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "        file.write(\"Subject To\\n\")\n",
    "        cnt = 0\n",
    "\n",
    "        # Constraint: Outgoing arcs from depot (customer 1) equal number of salesmen\n",
    "        cnt += 1\n",
    "        file.write(f\" c{cnt}:\" + \"\".join([\n",
    "            f\" + 1 x_1_{j+1}\" for j in range(1, n_customers)\n",
    "        ]) + f\" = {m_salesman}\\n\")\n",
    "\n",
    "        # Constraint: Incoming arcs to depot equal number of salesmen\n",
    "        cnt += 1\n",
    "        file.write(f\" c{cnt}:\" + \"\".join([\n",
    "            f\" + 1 x_{j+1}_1\" for j in range(1, n_customers)\n",
    "        ]) + f\" = {m_salesman}\\n\")\n",
    "\n",
    "        # For each customer (except depot), exactly one outgoing arc\n",
    "        for i in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([\n",
    "                f\" + 1 x_{i+1}_{j+1}\" for j in range(n_customers) if j+1 != i+1  # Exclude self-loops\n",
    "            ]) + \" = 1\\n\")\n",
    "\n",
    "        # For each customer (except depot), exactly one incoming arc\n",
    "        for j in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}:\" + \"\".join([\n",
    "                f\" + 1 x_{i+1}_{j+1}\" for i in range(n_customers) if i+1 != j+1  # Exclude self-loops\n",
    "            ]) + \" = 1\\n\")\n",
    "\n",
    "        # Fix the depot's u variable\n",
    "        cnt += 1\n",
    "        file.write(f\" c{cnt}: u_1 = 0\\n\")\n",
    "\n",
    "        # Constraints linking u and x (first set) - ensure proper route lengths\n",
    "        for i in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            L_minus_2 = L - 2\n",
    "            L_minus_1 = L - 1\n",
    "            file.write(f\" c{cnt}: u_{i+1} + {L_minus_2} x_1_{i+1} - x_{i+1}_1 <= {L_minus_1}\\n\")\n",
    "\n",
    "        # Constraints linking u and x (second set) - ensure proper route lengths\n",
    "        for i in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            two_minus_K = 2 - K\n",
    "            file.write(f\" c{cnt}: - u_{i+1} - x_1_{i+1} + {two_minus_K} x_{i+1}_1 <= 0\\n\")\n",
    "\n",
    "        # Constraint to ensure at most one of (depot -> i) and (i -> depot) is used\n",
    "        for i in range(1, n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}: x_1_{i+1} + x_{i+1}_1 <= 1\\n\")\n",
    "\n",
    "        # Sequencing constraints among customers (avoiding subtours)\n",
    "        for i in range(1, n_customers):\n",
    "            for j in range(1, n_customers):\n",
    "                if i == j:\n",
    "                    continue\n",
    "                cnt += 1\n",
    "                L_val = float(L)\n",
    "                L_minus_2_val = L - 2\n",
    "                L_minus_1_val = L - 1\n",
    "                file.write(f\" c{cnt}: {L_val} x_{i+1}_{j+1} + {L_minus_2_val} x_{j+1}_{i+1} + u_{i+1} - u_{j+1} <= {L_minus_1_val}\\n\")\n",
    "\n",
    "        # Self-loop constraints: no customer should visit itself\n",
    "        for i in range(n_customers):\n",
    "            cnt += 1\n",
    "            file.write(f\" c{cnt}: x_{i+1}_{i+1} = 0\\n\")\n",
    "\n",
    "        # Bounds section\n",
    "        file.write(\"\\nBounds\\n\")\n",
    "\n",
    "        # Bounds for u variables (for customers 2 to n_customers)\n",
    "        for i in range(1, n_customers):\n",
    "            file.write(f\" 1 <= u_{i+1} <= {n_customers - 1}\\n\")\n",
    "\n",
    "        # Declare general integer variables\n",
    "        file.write(\"\\nGenerals\\n\")\n",
    "        for i in range(1, n_customers):\n",
    "            file.write(f\" u_{i+1}\")\n",
    "\n",
    "        file.write(\"\\n\\nBinary\\n\")\n",
    "        # Declare binary variables for all arcs except self-loops (which are fixed to 0)\n",
    "        for i in range(n_customers):\n",
    "            for j in range(n_customers):\n",
    "                if i != j:  # Only declare non-self-loops as binary\n",
    "                    file.write(f\" x_{i+1}_{j+1}\")\n",
    "\n",
    "        file.write(\"\\nEnd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6Q-P8IiP4ks"
   },
   "outputs": [],
   "source": [
    "def generate_JSSP(rng, filename, n_jobs, n_machines, p_min=1, p_max=10, big_M=1000):\n",
    "    \"\"\"\n",
    "    Generates a Job Shop Scheduling problem in the CPLEX LP format.\n",
    "\n",
    "    In this instance, each of the n_jobs has n_machines operations. For every job,\n",
    "    a random permutation of machines is chosen (each job visits every machine in a\n",
    "    unique order) and each operation is assigned a random processing time between\n",
    "    p_min and p_max. The model uses a big-M formulation with binary variables to\n",
    "    enforce that on each machine the operations do not overlap.\n",
    "\n",
    "    The formulation includes:\n",
    "      - Precedence constraints for operations within each job:\n",
    "            1 S_{i,k+2} - 1 S_{i,k+1} >= p[i,k]\n",
    "      - Machine (disjunctive) constraints for every pair of operations on the same machine:\n",
    "            1 S_{i,k+1} - 1 S_{j,l+1} + big_M Y_{i,k,j,l} <= big_M - p[i,k]\n",
    "            1 S_{j,l+1} - 1 S_{i,k+1} - big_M Y_{i,k,j,l} <= - p[j,l]\n",
    "      - Makespan constraints for each operation (rewritten so the right-hand side is a constant):\n",
    "            1 S_{i,k+1} - 1 C_max <= - p[i,k]\n",
    "      - The objective is to minimize C_max.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rng : numpy.random.RandomState\n",
    "        Random number generator.\n",
    "    filename : str\n",
    "        File to which the LP will be written.\n",
    "    n_jobs : int\n",
    "        Number of jobs.\n",
    "    n_machines : int\n",
    "        Number of machines (and operations per job).\n",
    "    p_min : int, optional\n",
    "        Minimum processing time for an operation (default 1).\n",
    "    p_max : int, optional\n",
    "        Maximum processing time for an operation (default 10).\n",
    "    big_M : int, optional\n",
    "        Big-M constant used in disjunctive constraints (default 1000).\n",
    "    \"\"\"\n",
    "    # Generate processing times (n_jobs x n_machines)\n",
    "    processing_times = rng.randint(p_min, p_max+1, size=(n_jobs, n_machines))\n",
    "    # For each job, generate a random permutation of machines (each job visits each machine once)\n",
    "    machine_seq = [rng.permutation(n_machines).tolist() for _ in range(n_jobs)]\n",
    "\n",
    "    # List to store binary variable names for machine ordering constraints.\n",
    "    y_vars = []\n",
    "\n",
    "    with open(filename, 'w') as file:\n",
    "        # Write objective: minimize C_max.\n",
    "        file.write(\"minimize\\n\")\n",
    "        file.write(\" obj: C_max\\n\\n\")\n",
    "\n",
    "        file.write(\"subject to\\n\")\n",
    "        cnt = 1\n",
    "\n",
    "        # 1. Precedence constraints (operations must follow in order within each job).\n",
    "        for i in range(n_jobs):\n",
    "            for k in range(n_machines - 1):\n",
    "                p_val = processing_times[i, k]\n",
    "                # Write constraint as: 1 S_{i+1}_{k+2} - 1 S_{i+1}_{k+1} >= p_val\n",
    "                file.write(f\" c{cnt}: 1 S_{i+1}_{k+2} - 1 S_{i+1}_{k+1} >= {p_val}\\n\")\n",
    "                cnt += 1\n",
    "\n",
    "        # 2. Machine disjunctive constraints.\n",
    "        for m in range(n_machines):\n",
    "            ops = []  # List of tuples (job_index, op_index) for operations on machine m.\n",
    "            for i in range(n_jobs):\n",
    "                for k in range(n_machines):\n",
    "                    if machine_seq[i][k] == m:\n",
    "                        ops.append((i, k))\n",
    "            # For every pair of operations on the same machine.\n",
    "            for a in range(len(ops)):\n",
    "                for b in range(a+1, len(ops)):\n",
    "                    i, k = ops[a]\n",
    "                    j, l = ops[b]\n",
    "                    # Create a binary variable for the ordering.\n",
    "                    Y_name = f\"Y_{i+1}_{k+1}_{j+1}_{l+1}\"\n",
    "                    y_vars.append(Y_name)\n",
    "                    # Constraint (1):\n",
    "                    # Original: S_{i+1}_{k+1} + p[i,k] <= S_{j+1}_{l+1} + big_M*(1 - Y)\n",
    "                    # Rearranged: 1 S_{i+1}_{k+1} - 1 S_{j+1}_{l+1} + big_M Y <= big_M - p[i,k]\n",
    "                    file.write(f\" c{cnt}: 1 S_{i+1}_{k+1} - 1 S_{j+1}_{l+1} + {big_M} {Y_name} <= {big_M - processing_times[i, k]}\\n\")\n",
    "                    cnt += 1\n",
    "                    # Constraint (2):\n",
    "                    # Original: S_{j+1}_{l+1} + p[j,l] <= S_{i+1}_{k+1} + big_M*Y\n",
    "                    # Rearranged: 1 S_{j+1}_{l+1} - 1 S_{i+1}_{k+1} - big_M Y <= - p[j,l]\n",
    "                    file.write(f\" c{cnt}: 1 S_{j+1}_{l+1} - 1 S_{i+1}_{k+1} - {big_M} {Y_name} <= {-processing_times[j, l]}\\n\")\n",
    "                    cnt += 1\n",
    "\n",
    "        # 3. Makespan constraints.\n",
    "        # Original intended constraint: S_{i+1}_{k+1} + p[i,k] <= C_max\n",
    "        # Rewrite as: S_{i+1}_{k+1} - C_max <= - p[i,k]\n",
    "        for i in range(n_jobs):\n",
    "            for k in range(n_machines):\n",
    "                file.write(f\" c{cnt}: 1 S_{i+1}_{k+1} - 1 C_max <= -{processing_times[i, k]}\\n\")\n",
    "                cnt += 1\n",
    "\n",
    "        # 4. Bounds on variables.\n",
    "        file.write(\"\\nbounds\\n\")\n",
    "        # Lower bounds for start times.\n",
    "        for i in range(n_jobs):\n",
    "            for k in range(n_machines):\n",
    "                file.write(f\" 0 <= S_{i+1}_{k+1}\\n\")\n",
    "        # Lower bound for makespan.\n",
    "        file.write(\" 0 <= C_max\\n\")\n",
    "\n",
    "        # 5. Declare binary variables.\n",
    "        file.write(\"\\nbinary\\n\")\n",
    "        file.write(\" \" + \" \".join(y_vars) + \"\\n\")\n",
    "\n",
    "        file.write(\"\\nend\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHybuP6jPe_z"
   },
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elwCo9GicQ7Q"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        'problem',\n",
    "        help='MILP instance type to process.',\n",
    "        choices=['Standard_MTSP', 'MinMax_MTSP', 'Bounded_MTSP', 'JSSP'],\n",
    "        # choices=['MinMax_MTSP'],\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '-s', '--seed',\n",
    "        help='Random generator seed (default 0).',\n",
    "        type=valid_seed,\n",
    "        default=0,\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(['Standard_MTSP'])\n",
    "\n",
    "    rng = np.random.RandomState(args.seed)\n",
    "\n",
    "    if args.problem == 'Standard_MTSP':\n",
    "        number_of_customers = 12\n",
    "        number_of_salesman = 3\n",
    "        filenames = []\n",
    "        ncustomerss = []\n",
    "        nsalesmans = []\n",
    "\n",
    "        # train instances\n",
    "        n = 10000\n",
    "        lp_dir = f'data/instances/{args.problem}/train_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "\n",
    "        if os.path.exists(lp_dir):\n",
    "            shutil.rmtree(lp_dir)\n",
    "\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])   #extend() 函数用于在列表末尾一次性追加另一个序列中的多个值\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "\n",
    "        # validation instances\n",
    "        n = 2000\n",
    "        lp_dir = f'data/instances/{args.problem}/valid_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "\n",
    "        if os.path.exists(lp_dir):\n",
    "            shutil.rmtree(lp_dir)\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "\n",
    "        # test instances\n",
    "        n = 2000\n",
    "        lp_dir = f'data/instances/{args.problem}/test_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "\n",
    "        if os.path.exists(lp_dir):\n",
    "            shutil.rmtree(lp_dir)\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "\n",
    "        # actually generate the instances\n",
    "        for filename, ncs, nsm in zip(filenames, ncustomerss, nsalesmans): #zip() 函数用于将可迭代的对象作为参数，\n",
    "                                                                                        #将对象中对应的元素打包成一个个元组，然后返回由这些元组组成的列表\n",
    "            print(f\"  generating file {filename} ...\")\n",
    "            generate_Standard_MTSP(rng, filename, n_customers=ncs, m_salesman=nsm)\n",
    "\n",
    "        print(\"done.\")\n",
    "\n",
    "    elif args.problem == 'MinMax_MTSP':\n",
    "        number_of_customers = 12\n",
    "        number_of_salesman = 3\n",
    "        filenames = []\n",
    "        ncustomerss = []\n",
    "        nsalesmans = []\n",
    "\n",
    "        # train instances\n",
    "        n = 10000\n",
    "        lp_dir = f'data/instances/MinMax_MTSP/train_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])   #extend() 函数用于在列表末尾一次性追加另一个序列中的多个值\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "\n",
    "        # validation instances\n",
    "        n = 2000\n",
    "        lp_dir = f'data/instances/MinMax_MTSP/valid_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "\n",
    "        # test instances\n",
    "        n = 2000\n",
    "        number_of_customers = 9\n",
    "        number_of_salesman = 3\n",
    "        lp_dir = f'data/instances/MinMax_MTSP/test_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "\n",
    "        # actually generate the instances\n",
    "        for filename, ncs, nsm in zip(filenames, ncustomerss, nsalesmans): \n",
    "            \n",
    "            print(f\"  generating file {filename} ...\")\n",
    "            generate_MinMax_MTSP(rng, filename, n_customers=ncs, m_salesman=nsm)\n",
    "\n",
    "        print(\"done.\")\n",
    "\n",
    "    elif args.problem == 'Bounded_MTSP':\n",
    "        number_of_customers = 12\n",
    "        number_of_salesman = 3\n",
    "        max_num = 6\n",
    "        min_num = 2\n",
    "        filenames = []\n",
    "        ncustomerss = []\n",
    "        nsalesmans = []\n",
    "        n_L = []\n",
    "        n_K = []\n",
    "        # train instances\n",
    "        n = 10000\n",
    "        lp_dir = f'data/instances/Bounded_MTSP/train_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        if os.path.exists(lp_dir):\n",
    "            shutil.rmtree(lp_dir)\n",
    "\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])   \n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "        n_L.extend([max_num] * n)\n",
    "        n_K.extend([min_num] * n)\n",
    "\n",
    "        # validation instances\n",
    "        n = 2000\n",
    "        lp_dir = f'data/instances/Bounded_MTSP/valid_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        if os.path.exists(lp_dir):\n",
    "            shutil.rmtree(lp_dir)\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "        n_L.extend([max_num] * n)\n",
    "        n_K.extend([min_num] * n)\n",
    "\n",
    "        # test instances\n",
    "        n = 2000\n",
    "        number_of_customers = 12\n",
    "        number_of_salesman = 3\n",
    "        lp_dir = f'data/instances/Bounded_MTSP/test_{number_of_customers}_{number_of_salesman}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        if os.path.exists(lp_dir):\n",
    "            shutil.rmtree(lp_dir)\n",
    "        os.makedirs(lp_dir)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        ncustomerss.extend([number_of_customers] * n)\n",
    "        nsalesmans.extend([number_of_salesman] * n)\n",
    "        n_L.extend([max_num] * n)\n",
    "        n_K.extend([min_num] * n)\n",
    "\n",
    "        # actually generate the instances\n",
    "        for filename, ncs, nsm, L, K in zip(filenames, ncustomerss, nsalesmans, n_L, n_K): \n",
    "                                                                                        \n",
    "            print(f\"  generating file {filename} ...\")\n",
    "            generate_Bounded_MTSP(rng, filename, n_customers=ncs, m_salesman=nsm, L=L, K=K)\n",
    "\n",
    "        print(\"done.\")\n",
    "\n",
    "    elif args.problem == 'JSSP':\n",
    "        # Increase the number of jobs and machines for a harder instance.\n",
    "        number_of_jobs = 8      \n",
    "        number_of_machines = 6  \n",
    "        filenames = []\n",
    "        njobs_list = []\n",
    "        nmachines_list = []\n",
    "\n",
    "        # train instances\n",
    "        n = 10000\n",
    "        lp_dir = f'data/instances/JSSP/train_{number_of_jobs}_{number_of_machines}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        os.makedirs(lp_dir, exist_ok=True)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        njobs_list.extend([number_of_jobs] * n)\n",
    "        nmachines_list.extend([number_of_machines] * n)\n",
    "\n",
    "        # validation instances\n",
    "        n = 2000\n",
    "        lp_dir = f'data/instances/JSSP/valid_{number_of_jobs}_{number_of_machines}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        os.makedirs(lp_dir, exist_ok=True)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        njobs_list.extend([number_of_jobs] * n)\n",
    "        nmachines_list.extend([number_of_machines] * n)\n",
    "\n",
    "        # test instances\n",
    "        n = 2000\n",
    "        lp_dir = f'data/instances/JSSP/test_{number_of_jobs}_{number_of_machines}'\n",
    "        print(f\"{n} instances in {lp_dir}\")\n",
    "        os.makedirs(lp_dir, exist_ok=True)\n",
    "        filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "        njobs_list.extend([number_of_jobs] * n)\n",
    "        nmachines_list.extend([number_of_machines] * n)\n",
    "\n",
    "        # Actually generate the instances using generate_job_shop_scheduling.\n",
    "        for filename, nj, nm in zip(filenames, njobs_list, nmachines_list):\n",
    "            print(f\"  generating file {filename} ...\")\n",
    "            generate_JSSP(rng, filename, n_jobs=nj, n_machines=nm, p_min=1, p_max=10, big_M=1000)\n",
    "\n",
    "        print(\"done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 706336,
     "status": "ok",
     "timestamp": 1735234762405,
     "user": {
      "displayName": "Toan Nguyen",
      "userId": "09584472438722923742"
     },
     "user_tz": -420
    },
    "id": "5tMjeCVfMI16",
    "outputId": "88fb6793-3ad5-4f44-bd45-3adaf34496ff"
   },
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     parser.add_argument(\n",
    "#         'problem',\n",
    "#         help='MILP instance type to process.',\n",
    "#         #choices=['setcover', 'cauctions', 'facilities', 'indset'],\n",
    "#         choices=['facilities'],\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         '-s', '--seed',\n",
    "#         help='Random generator seed (default 0).',\n",
    "#         type=valid_seed,\n",
    "#         default=0,\n",
    "#     )\n",
    "#     args = parser.parse_args(['facilities'])\n",
    "\n",
    "#     rng = np.random.RandomState(args.seed)\n",
    "\n",
    "#     if args.problem == 'setcover':\n",
    "#         nrows = 500\n",
    "#         ncols = 1000\n",
    "#         dens = 0.05\n",
    "#         max_coef = 100\n",
    "\n",
    "#         filenames = []\n",
    "#         nrowss = []\n",
    "#         ncolss = []\n",
    "#         denss = []\n",
    "\n",
    "#         # train instances\n",
    "#         n = 10000\n",
    "#         lp_dir = f'data/instances/setcover/train_{nrows}r_{ncols}c_{dens}d'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nrowss.extend([nrows] * n)\n",
    "#         ncolss.extend([ncols] * n)\n",
    "#         denss.extend([dens] * n)\n",
    "\n",
    "#         # validation instances\n",
    "#         n = 2000\n",
    "#         lp_dir = f'data/instances/setcover/valid_{nrows}r_{ncols}c_{dens}d'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nrowss.extend([nrows] * n)\n",
    "#         ncolss.extend([ncols] * n)\n",
    "#         denss.extend([dens] * n)\n",
    "\n",
    "#         # small transfer instances\n",
    "#         n = 100\n",
    "#         nrows = 500\n",
    "#         lp_dir = f'data/instances/setcover/transfer_{nrows}r_{ncols}c_{dens}d'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nrowss.extend([nrows] * n)\n",
    "#         ncolss.extend([ncols] * n)\n",
    "#         denss.extend([dens] * n)\n",
    "\n",
    "#         # medium transfer instances\n",
    "#         n = 100\n",
    "#         nrows = 1000\n",
    "#         lp_dir = f'data/instances/setcover/transfer_{nrows}r_{ncols}c_{dens}d'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nrowss.extend([nrows] * n)\n",
    "#         ncolss.extend([ncols] * n)\n",
    "#         denss.extend([dens] * n)\n",
    "\n",
    "#         # big transfer instances\n",
    "#         n = 100\n",
    "#         nrows = 2000\n",
    "#         lp_dir = f'data/instances/setcover/transfer_{nrows}r_{ncols}c_{dens}d'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nrowss.extend([nrows] * n)\n",
    "#         ncolss.extend([ncols] * n)\n",
    "#         denss.extend([dens] * n)\n",
    "\n",
    "#         # test instances\n",
    "#         n = 2000\n",
    "#         nrows = 500\n",
    "#         ncols = 1000\n",
    "#         lp_dir = f'data/instances/setcover/test_{nrows}r_{ncols}c_{dens}d'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nrowss.extend([nrows] * n)\n",
    "#         ncolss.extend([ncols] * n)\n",
    "#         denss.extend([dens] * n)\n",
    "\n",
    "#         # actually generate the instances\n",
    "#         for filename, nrows, ncols, dens in zip(filenames, nrowss, ncolss, denss):\n",
    "#             print(f'  generating file {filename} ...')\n",
    "#             generate_setcover(nrows=nrows, ncols=ncols, density=dens, filename=filename, rng=rng, max_coef=max_coef)\n",
    "\n",
    "#         print('done.')\n",
    "\n",
    "#     elif args.problem == 'indset':\n",
    "#         number_of_nodes = 500\n",
    "#         affinity = 4\n",
    "\n",
    "#         filenames = []\n",
    "#         nnodess = []\n",
    "\n",
    "#         # train instances\n",
    "#         n = 10000\n",
    "#         lp_dir = f'data/instances/indset/train_{number_of_nodes}_{affinity}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nnodess.extend([number_of_nodes] * n)\n",
    "\n",
    "#         # validation instances\n",
    "#         n = 2000\n",
    "#         lp_dir = f'data/instances/indset/valid_{number_of_nodes}_{affinity}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nnodess.extend([number_of_nodes] * n)\n",
    "\n",
    "#         # small transfer instances\n",
    "#         n = 100\n",
    "#         number_of_nodes = 500\n",
    "#         lp_dir = f'data/instances/indset/transfer_{number_of_nodes}_{affinity}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nnodess.extend([number_of_nodes] * n)\n",
    "\n",
    "#         # medium transfer instances\n",
    "#         n = 100\n",
    "#         number_of_nodes = 1000\n",
    "#         lp_dir = f'data/instances/indset/transfer_{number_of_nodes}_{affinity}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nnodess.extend([number_of_nodes] * n)\n",
    "\n",
    "#         # big transfer instances\n",
    "#         n = 100\n",
    "#         number_of_nodes = 1500\n",
    "#         lp_dir = f'data/instances/indset/transfer_{number_of_nodes}_{affinity}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nnodess.extend([number_of_nodes] * n)\n",
    "\n",
    "#         # test instances\n",
    "#         n = 2000\n",
    "#         number_of_nodes = 500\n",
    "#         lp_dir = f'data/instances/indset/test_{number_of_nodes}_{affinity}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nnodess.extend([number_of_nodes] * n)\n",
    "\n",
    "#         # actually generate the instances\n",
    "#         for filename, nnodes in zip(filenames, nnodess):\n",
    "#             print(f\"  generating file {filename} ...\")\n",
    "#             graph = Graph.barabasi_albert(nnodes, affinity, rng)\n",
    "#             generate_indset(graph, filename)\n",
    "\n",
    "#         print(\"done.\")\n",
    "\n",
    "#     elif args.problem == 'cauctions':\n",
    "#         number_of_items = 100\n",
    "#         number_of_bids = 500\n",
    "#         filenames = []\n",
    "#         nitemss = []\n",
    "#         nbidss = []\n",
    "\n",
    "#         # train instances\n",
    "#         n = 10000\n",
    "#         lp_dir = f'data/instances/cauctions/train_{number_of_items}_{number_of_bids}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nitemss.extend([number_of_items] * n)\n",
    "#         nbidss.extend([number_of_bids ] * n)\n",
    "\n",
    "#         # validation instances\n",
    "#         n = 2000\n",
    "#         lp_dir = f'data/instances/cauctions/valid_{number_of_items}_{number_of_bids}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nitemss.extend([number_of_items] * n)\n",
    "#         nbidss.extend([number_of_bids ] * n)\n",
    "\n",
    "#         # small transfer instances\n",
    "#         n = 100\n",
    "#         number_of_items = 100\n",
    "#         number_of_bids = 500\n",
    "#         lp_dir = f'data/instances/cauctions/transfer_{number_of_items}_{number_of_bids}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nitemss.extend([number_of_items] * n)\n",
    "#         nbidss.extend([number_of_bids ] * n)\n",
    "\n",
    "#         # medium transfer instances\n",
    "#         n = 100\n",
    "#         number_of_items = 200\n",
    "#         number_of_bids = 1000\n",
    "#         lp_dir = f'data/instances/cauctions/transfer_{number_of_items}_{number_of_bids}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nitemss.extend([number_of_items] * n)\n",
    "#         nbidss.extend([number_of_bids ] * n)\n",
    "\n",
    "#         # big transfer instances\n",
    "#         n = 100\n",
    "#         number_of_items = 300\n",
    "#         number_of_bids = 1500\n",
    "#         lp_dir = f'data/instances/cauctions/transfer_{number_of_items}_{number_of_bids}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nitemss.extend([number_of_items] * n)\n",
    "#         nbidss.extend([number_of_bids ] * n)\n",
    "\n",
    "#         # test instances\n",
    "#         n = 2000\n",
    "#         number_of_items = 100\n",
    "#         number_of_bids = 500\n",
    "#         lp_dir = f'data/instances/cauctions/test_{number_of_items}_{number_of_bids}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         nitemss.extend([number_of_items] * n)\n",
    "#         nbidss.extend([number_of_bids ] * n)\n",
    "\n",
    "#         # actually generate the instances\n",
    "#         for filename, nitems, nbids in zip(filenames, nitemss, nbidss):\n",
    "#             print(f\"  generating file {filename} ...\")\n",
    "#             generate_cauctions(rng, filename, n_items=nitems, n_bids=nbids, add_item_prob=0.7)\n",
    "\n",
    "#         print(\"done.\")\n",
    "\n",
    "#     elif args.problem == 'facilities':\n",
    "#         number_of_customers = 100\n",
    "#         number_of_facilities = 100\n",
    "#         ratio = 5\n",
    "#         filenames = []\n",
    "#         ncustomerss = []\n",
    "#         nfacilitiess = []\n",
    "#         ratios = []\n",
    "\n",
    "#         # train instances\n",
    "#         n = 10000\n",
    "#         lp_dir = f'data/instances/facilities/train_{number_of_customers}_{number_of_facilities}_{ratio}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         ncustomerss.extend([number_of_customers] * n)\n",
    "#         nfacilitiess.extend([number_of_facilities] * n)\n",
    "#         ratios.extend([ratio] * n)\n",
    "\n",
    "#         # validation instances\n",
    "#         n = 2000\n",
    "#         lp_dir = f'data/instances/facilities/valid_{number_of_customers}_{number_of_facilities}_{ratio}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         ncustomerss.extend([number_of_customers] * n)\n",
    "#         nfacilitiess.extend([number_of_facilities] * n)\n",
    "#         ratios.extend([ratio] * n)\n",
    "\n",
    "#         # small transfer instances\n",
    "#         n = 100\n",
    "#         number_of_customers = 100\n",
    "#         number_of_facilities = 100\n",
    "#         lp_dir = f'data/instances/facilities/transfer_{number_of_customers}_{number_of_facilities}_{ratio}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         ncustomerss.extend([number_of_customers] * n)\n",
    "#         nfacilitiess.extend([number_of_facilities] * n)\n",
    "#         ratios.extend([ratio] * n)\n",
    "\n",
    "#         # medium transfer instances\n",
    "#         n = 100\n",
    "#         number_of_customers = 200\n",
    "#         lp_dir = f'data/instances/facilities/transfer_{number_of_customers}_{number_of_facilities}_{ratio}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         ncustomerss.extend([number_of_customers] * n)\n",
    "#         nfacilitiess.extend([number_of_facilities] * n)\n",
    "#         ratios.extend([ratio] * n)\n",
    "\n",
    "#         # big transfer instances\n",
    "#         n = 100\n",
    "#         number_of_customers = 400\n",
    "#         lp_dir = f'data/instances/facilities/transfer_{number_of_customers}_{number_of_facilities}_{ratio}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         ncustomerss.extend([number_of_customers] * n)\n",
    "#         nfacilitiess.extend([number_of_facilities] * n)\n",
    "#         ratios.extend([ratio] * n)\n",
    "\n",
    "#         # test instances\n",
    "#         n = 2000\n",
    "#         number_of_customers = 100\n",
    "#         number_of_facilities = 100\n",
    "#         lp_dir = f'data/instances/facilities/test_{number_of_customers}_{number_of_facilities}_{ratio}'\n",
    "#         print(f\"{n} instances in {lp_dir}\")\n",
    "#         os.makedirs(lp_dir)\n",
    "#         filenames.extend([os.path.join(lp_dir, f'instance_{i+1}.lp') for i in range(n)])\n",
    "#         ncustomerss.extend([number_of_customers] * n)\n",
    "#         nfacilitiess.extend([number_of_facilities] * n)\n",
    "#         ratios.extend([ratio] * n)\n",
    "\n",
    "#         # actually generate the instances\n",
    "#         for filename, ncs, nfs, r in zip(filenames, ncustomerss, nfacilitiess, ratios):\n",
    "#             print(f\"  generating file {filename} ...\")\n",
    "#             generate_capacited_facility_location(rng, filename, n_customers=ncs, n_facilities=nfs, ratio=r)\n",
    "\n",
    "#         print(\"done.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
